[{"categories":["Azure","Governance","Infrastructure as Code","Azure Service Groups"],"content":"Learn how to create and manage Azure Service Groups using Bicep Infrastructure as Code, enabling consistent, repeatable, and auditable resource organization across your tenant.","date":"2025-12-19","objectID":"/posts/2025/azure-service-groups-as-code-vending-the-multiverse-with-bicep/","tags":["Azure Governance","Azure Service Groups","Infrastructure as Code","Bicep","Azure Automation"],"title":"üìê Azure Service Groups as Code: Vending the Multiverse with Bicep üåå","uri":"/posts/2025/azure-service-groups-as-code-vending-the-multiverse-with-bicep/"},{"categories":["Azure","Governance","Infrastructure as Code","Azure Service Groups"],"content":"In my previous post, Azure Service Groups: The Multiverse of Resource Organization, we explored what Azure Service Groups are and why they matter when organizing resources across an Azure tenant. That post also introduced a set of core Service Group fundamentals. While all of those fundamentals are important, three of them are critical when you want to implement Azure Service Groups at scale. In this post, I‚Äôll focus on the first of those three and show how to put it into practice using Bicep and Infrastructure as Code (IaC): standardizing your approach early. The three fundamentals are: Standardize your approach early Have the platform team design the initial top-level Service Group hierarchy, including naming, structure, and topology. Ownership of deeper layers can then be delegated to enabling teams and domain experts, allowing them to model according to business needs. Shift left through automation Apply Service Group membership at deployment time and treat it as a first-class part of your resource provisioning process, not an afterthought. Use Azure Policy for scale Use policy assignments to automate Service Group membership, ensuring consistency, reducing human error, and guaranteeing that new resources always land in the correct logical groups. Azure Service Groups are currently in preview\rAt the time of writing, Azure Service Groups are in public preview. Treat them accordingly and expect the feature to evolve before it reaches general availability.\rStandardize your approach early\rThe core non-functional requirements here are standardization and flexibility. To meet both, we can reuse a familiar and proven approach: the management group vending pattern, as described in Simplify Azure Management Group Setup Using Bicep Vending, and apply it to Azure Service Groups. Standardization and flexibility\rFollowing this pattern, the goal is for the platform team to define and deploy tier 01 and tier 02 Service Groups. To ensure the flexibility requirement is met, the vending solution supports a total of 10 tiers, even though this example only demonstrates up to tier 02. How deep the initial top-level Service Group hierarchy should go depends on your organization‚Äôs needs and operating model.\rAs a starting point, let‚Äôs reuse the example Service Group hierarchy from the Azure Service Groups: The Multiverse of Resource Organization post: Service Groups ‚Äì Example Hierarchy\rNext, we slice this hierarchy into tiers, following the same principles as the management group vending pattern: Service Groups ‚Äì Tiered Pattern\rTier 00 The root Service Group. This is present by default in every Azure tenant. Tier 01 The first level of the top-level Service Group hierarchy. Managed by the platform team. Tier 02 The second level of the top-level Service Group hierarchy. Managed by the platform team. Tier 03 ‚Äì Tier 10 Deeper levels of the Service Group hierarchy, owned and managed by enabling teams and domain experts. Repurposing the Management Group Vending Pattern\rThe solution is composed of three building blocks: Resource module Solution module Parameter file Resource Module\rThe resource module is the core building block of the solution. It uses the Microsoft.Management/serviceGroups@2024-02-01-preview resource type to create a single Service Group. Because Service Groups are tenant-level resources, the module is scoped accordingly using targetScope = 'tenant'. Azure Verified Modules\rThe snippet above shows a deliberately simple resource module that creates a single Service Group and is intended for demonstration and learning purposes. For enterprise-grade production environments, it is strongly recommended to use the tested and maintained modules available in the Azure Verified Modules repository. For Service Groups, this is the Azure Verified Module for Service Groups. Solution Module\rThe solution module acts as the orchestrator. It consumes the resource module and deploys the Service Group hierarchy tier by tier. Each tier is modeled as a coll","date":"2025-12-19","objectID":"/posts/2025/azure-service-groups-as-code-vending-the-multiverse-with-bicep/:0:0","tags":["Azure Governance","Azure Service Groups","Infrastructure as Code","Bicep","Azure Automation"],"title":"üìê Azure Service Groups as Code: Vending the Multiverse with Bicep üåå","uri":"/posts/2025/azure-service-groups-as-code-vending-the-multiverse-with-bicep/"},{"categories":["Azure","Governance","Architecture"],"content":"Discover how Azure Service Groups introduce a flexible, cross-subscription layer of organization that redefines how you manage resources across your tenant.","date":"2025-12-09","objectID":"/posts/2025/azure-service-groups-the-multiverse-of-resource-organization/","tags":["Azure Governance","Azure Service Groups","Azure Management","Resource Organization","Cloud Architecture"],"title":"üß© Azure Service Groups: The Multiverse of Resource Organization üõ∏","uri":"/posts/2025/azure-service-groups-the-multiverse-of-resource-organization/"},{"categories":["Azure","Governance","Architecture"],"content":"Hierarchial challenges in Azure\rMost organizations that have been using Azure follow some variation of the same hierarchical structure. From the top down, the hierarchy begins with Management Groups, which handle governance, policy, and compliance. Below them are Subscriptions, the natural boundaries for resource management, billing, and access control. At the bottom are Resource Groups, which organize resources according to application lifecycle or deployment boundaries. Typical Azure Hierarchial Structure\rWhile each of these constructs has its own purpose, the hierarchy itself is singular and rigid. Resource Groups must live inside Subscriptions, Subscriptions must live inside Management Groups, and Management Groups can only containSubscriptions or other Management Groups. Every organization using Azure must operate within this rigid structure while still trying to meet its own organizational needs. To achieve this teams do their best to add context for cost management, resiliency, monitoring, and persona-specific requirements. Tools such as Azure Resource Graph, Resource Tags, and Azure Policy help deliver specific insights, but none of them provide a flexible way to organize and manage resources across the hierarchy. This gap is exactly why Azure Service Groups were introduced. Azure Service Groups are currently in preview\rAt the time of writing, Azure Service Groups are in public preview. Treat them accordingly and expect the feature to evolve before it reaches general availability.\rWhat are Azure Service Groups?\rAzure Service Groups are Tenant level logical containers aimed at creating flexible hierarchy to groups resources, resource groups and subscriptions across a tenant and across all current hierarchial boundaries. Service Groups exists independent an parallel to the existing hierarchy. As highlighted in the image below. Service Groups - One Service with multitude of scenario‚Äôs\rTenant Level\rUnlike existing hierarchy constructs, Service Groups do not support RBAC or policy inheritance. Permissions assigned at one level of the Service Group hierarchy do not cascade to any groups below it. This makes Service Groups inherently least-privilege and well-suited for persona-based access models.\rKey features\rService Groups are built around three core capabilities: hierarchy, flexibility, and aggregation. Together, these enable a level of organizational modeling that Azure‚Äôs traditional structure cannot provide. Hierarchy Supports branching structures and parallel grouping models. Allows self-nesting of Service Groups, up to 10 levels deep. Flexibility Members can come from any subscription within the tenant. Supports grouping of resources, resource groups, and subscriptions. A single resource can belong to multiple Service Groups simultaneously. Aggregation Consolidates data, insights, and telemetry from all included members‚Äîregardless of subscription boundaries. Enables visualizing workload- or domain-specific views from the perspective you choose. Available today\rCurrently available features for Service Groups. Portal an copilot experiences REST API for Microsoft.Management and Microsoft.Relationship ARM, Bicep and Terraform support Resource Graph support Monitoring and recommendation views I encourage anyone to open their own tenant and explore the currently available experiences as mentioned above. However for those do not have a tenant at hand. Here are some screenshots of the current portal experience. Service Groups - Monitoring View\rService Groups - Coverage + Recommendation View\rPlanned\rPlanned features for Service Groups. Expanded SDK support (powerShell, CLI, etc) Native condition based membership Additional relationship types, like dependencies Cost management Experience - (Personally looking forward to this one the most) Policy Compliance Experience General Availability Limitations\rA maximum of 2000 service group members per subscription During preview the limit is 10000 Service Groups per single tenant Service","date":"2025-12-09","objectID":"/posts/2025/azure-service-groups-the-multiverse-of-resource-organization/:0:0","tags":["Azure Governance","Azure Service Groups","Azure Management","Resource Organization","Cloud Architecture"],"title":"üß© Azure Service Groups: The Multiverse of Resource Organization üõ∏","uri":"/posts/2025/azure-service-groups-the-multiverse-of-resource-organization/"},{"categories":["Cloud Center of Excellence","Cloud Strategy","Cloud Governance","Cloud Adoption Framework","Digital Transformation","Pathfinder Phase"],"content":"The Scale Stage is where momentum becomes maturity. Learn how to consolidate practices, enable reuse, and continuously improve your CCoE with standardization, validation, and repeatability.","date":"2025-07-04","objectID":"/posts/2025/building-a-cloud-center-of-excellence-the-scale-stage/","tags":["Cloud Center of Excellence (CCoE)","Cloud Adoption","Cloud Strategy","Scaling","Cloud Governance","Digital Transformation","Cloud Best Practices"],"title":"üìàüîÅ Building a Cloud Center of Excellence: The Scale Stage üß±üöÄ","uri":"/posts/2025/building-a-cloud-center-of-excellence-the-scale-stage/"},{"categories":["Cloud Center of Excellence","Cloud Strategy","Cloud Governance","Cloud Adoption Framework","Digital Transformation","Pathfinder Phase"],"content":"You‚Äôve Envisioned the mission, Aligned stakeholders, and Launched your first sprint. Now it‚Äôs time to Scale. New here? Start with the Pathfinder Journey overview, then explore each stage: Envision Stage Align Stage Launch Stage If you‚Äôre all caught up, let‚Äôs dive in. What Is the Scale Stage?\rThe Scale Stage turns validated ideas into organization-wide capabilities. Your Cloud Center of Excellence (CCoE) shifts from pioneering to a pattern of excellence: harvesting lessons learned, codifying them, and making them easily consumable by every new team. Put simply, Scale is where momentum evolves into maturity. üî• Catalyst: Ignite the Cycle of Change\rAt Scale, your CCoE becomes a catalyst for transformation, not just for one project but for every project that follows. Anchor your operations to the well-known PDCA loop: Plan for scale and structure Do by executing in small, safe batches Check outcomes and gather feedback Act on lessons learned‚Äîthen re-iterate from the Plan again This continuous-improvement rhythm keeps innovation flowing while guarding against drift. üß± Consolidation: Codify and Share What Works\rTo scale effectively, consolidate and codify what‚Äôs already working: Patterns in practice ‚Äì Identify and document repeatable approaches across teams Scaling standardization ‚Äì Establish guardrails, naming conventions, templates, and IaC modules Product service catalog ‚Äì Formalize what your platform team offers to app teams Inner-source ‚Äì Empower teams to contribute back into shared repositories Private marketplace ‚Äì Publish a curated, self-service catalog of compliant, reusable cloud assets That‚Äôs how one-off wins become a pattern of excellence. üß™ Pre-Mortem: Reflect and Prepare to Scale\rPost-mortems analyze what happened to the patient after they passed. Do a pre-mortem to evaluate the scaling approach assuming things will fail. Base this assumption on your lessons learned embedding a CCoE into your organization up to this point. Excellence by Design. Before onboarding the next business unit, pause to reflect and adapt: Run a retrospective spanning the Envision, Align, and Launch stages Capture challenges, gaps, and wins Document patterns as playbooks, templates, reusable modules, and knowledge-base articles Goal: make the next launch faster and smoother by handing over a proven kit-of-parts. üîÅ Validate and Apply Upstream First\rScaling isn‚Äôt copy-paste; it‚Äôs measured reuse. Apply lessons upstream‚Äîwhere patterns are authored by the source of authority: Refine base templates, policies, and guidance Feed field feedback back into the foundation Make improvements visible, versioned, and test-driven Every new team should benefit from every previous lesson. üîÑ Rinse and Repeat\rScale isn‚Äôt a finish line; it‚Äôs a rhythm. You‚Äôll keep onboarding new units, refreshing standards, updating reusable components, and mining insights. Over time, your CCoE becomes a platform of platforms‚Äîserving teams of different maturity levels yet all leveraging the same validated core. Repeat until you reach a stable, self-sustaining state of excellence. Key Takeaways\rScale = platform: Transform the CCoE from project to platform. Codify patterns: Turn proven practices into versioned, reusable building blocks. Run PDCA loops: Plan ‚Üí Do ‚Üí Check ‚Üí Act for continuous improvement. Contribute upstream: Refine templates and guardrails at the source. Iterate relentlessly: Each onboarding cycle should be faster and safer than the last. What‚Äôs Next?\rReady to graduate from the Pathfinder Phase? üëâ The upcoming Execution Phase post will cover how to embed your CCoE for long-term, enterprise-wide impact (coming soon). Wrapping up\rIf you found this post useful, be sure to explore the reference materials that inspired it: Gartner. Cloud Center of Excellence: Key to Cloud Success (2024) Microsoft Azure. Cloud Adoption Framework ‚Äì Govern and Manage (2025) Skelton \u0026 Pais. Team Topologies (2019) How to Build a Successful CCoE: A Step-by-Step Guide How to build a cloud center of e","date":"2025-07-04","objectID":"/posts/2025/building-a-cloud-center-of-excellence-the-scale-stage/:0:0","tags":["Cloud Center of Excellence (CCoE)","Cloud Adoption","Cloud Strategy","Scaling","Cloud Governance","Digital Transformation","Cloud Best Practices"],"title":"üìàüîÅ Building a Cloud Center of Excellence: The Scale Stage üß±üöÄ","uri":"/posts/2025/building-a-cloud-center-of-excellence-the-scale-stage/"},{"categories":["Cloud Center of Excellence","Cloud Strategy","Cloud Governance","Cloud Adoption Framework","Digital Transformation","Pathfinder Phase"],"content":"In the Launch Stage of your Cloud Center of Excellence journey, strategy turns into action. Learn how to run pre-mortems, define epics, refine your backlog, and deliver your first sprint with confidence.","date":"2025-04-29","objectID":"/posts/2025/building-a-cloud-center-of-excellence-the-launch-stage/","tags":["Cloud Center of Excellence","CCoE","Cloud Strategy","Cloud Governance","Cloud Adoption","Organizational Change","Team Topologies","Azure","Enablement","Excellence","Cloud","Microsoft"],"title":"üöÄ Building a Cloud Center of Excellence: The Launch Stage üîßüì¶","uri":"/posts/2025/building-a-cloud-center-of-excellence-the-launch-stage/"},{"categories":["Cloud Center of Excellence","Cloud Strategy","Cloud Governance","Cloud Adoption Framework","Digital Transformation","Pathfinder Phase"],"content":"So far in our Building a Cloud Center of Excellence series, we‚Äôve covered the groundwork: In The Pathfinder Journey, we set the stage for the transformation ahead, breaking down the roadmap to cloud maturity. In The Envision Stage, we defined the why behind the CCoE, clarifying the vision, building a common frame of reference, and securing leadership buy-in. In The Align Stage, we tuned the instruments and synchronized the team‚Äîestablishing clear accountabilities, shaping enablers, and aligning delivery around a common tempo. We have envisioned where we want to go. We have aligned our stakeholders, mapped out the big rocks, and secured leadership buy-in. Now it‚Äôs time to light the engines. The post-it notes are on the wall. The backlog is taking shape. The team is ready. Welcome to the Launch Stage. Where we stop planning and start building. üß≠ What Is the Launch Stage?\rThe Launch Stage is where our CCoE moves from thinking to doing. It‚Äôs the moment where alignment turns into execution. Plans on paper become real work. This is also when the Pathfinder team hands over the baton. Some members take on permanent roles within the CCoE. Others have completed their mission‚Äîtheir journey ends here. Like any agile journey, the goal isn‚Äôt to deliver everything at once. It‚Äôs to build momentum. To get moving, deliver value, and create a rhythm that can be improved with every sprint. Think of the Launch Stage like choosing the right instruments for an orchestra. Before we can play, we need to select the tools, arrange the sheet music, and agree on the tempo. üîÆ Run a Pre-mortem\rBefore diving into sprint planning, pause. The Launch Stage is about momentum, but building speed without awareness is dangerous. A pre-mortem helps the CCoE team to see and avoid the potholes down the road. In a pre-mortem workshop, gather the CCoE team and key stakeholders. Ask a simple but powerful question: üëâ ‚ÄúImagine it‚Äôs six months from now and this initiative has failed. What went wrong?‚Äù This exercise surfaces risks that might otherwise stay hidden. üí¨ Common patterns often emerge: ‚ÄúNobody really owns the work.‚Äù ‚ÄúWe are doing this next to our day jobs.‚Äù ‚ÄúWe don‚Äôt have the right access or permissions.‚Äù ‚ÄúWe are blocked because we‚Äôre waiting on other teams.‚Äù Document every risk, group related concerns, and make them visible to everyone involved. The goal isn‚Äôt to solve every problem today. It‚Äôs to create awareness, so the CCoE team can navigate obstacles‚Äînot be surprised by them mid-sprint. If alignment provides a shared direction, the pre-mortem provides a shared realism. Both are essential before moving into execution. üß± Define Workshop: Shape Epics and Features\rThis is where the CCoE backlog is born. The first step is to translate strategic goals into Epics and Features. This forms the foundation of the CCoE backlog and becomes the engine behind the first sprints. Each Epic should connect back to the outcomes aligned on earlier: platform enablement, security guardrails, developer experience, cost management, and more. This isn‚Äôt just about building technical components; it‚Äôs about enabling the organization to move faster, safer, and smarter in the cloud. If you‚Äôre unfamiliar with how Epics, Features, and Stories fit together, check out this guide on Azure DevOps work items for a quick overview. üéØ Pro tips: Initial collaboration works best in person. Run a physical brown paper session to map out Epics and Features together. Once aligned, digitize the results into a tool like Miro, Azure DevOps Delivery Plans, or whichever tool fits best. Keep business outcomes visible. Every Epic should tie back to a clear why. Avoid over-engineering at this stage. Detailed refinement will happen later during the next workshops. This phase is about creating structure without losing momentum‚Äîand setting the scaffolding that the CCoE‚Äôs future delivery will stand on. ‚úÇÔ∏è Refinement Workshop: User Stories and Tasks\rOnce Epics and Features are in place, it‚Äôs time to make the work exec","date":"2025-04-29","objectID":"/posts/2025/building-a-cloud-center-of-excellence-the-launch-stage/:0:0","tags":["Cloud Center of Excellence","CCoE","Cloud Strategy","Cloud Governance","Cloud Adoption","Organizational Change","Team Topologies","Azure","Enablement","Excellence","Cloud","Microsoft"],"title":"üöÄ Building a Cloud Center of Excellence: The Launch Stage üîßüì¶","uri":"/posts/2025/building-a-cloud-center-of-excellence-the-launch-stage/"},{"categories":["Cloud Center of Excellence","Cloud Strategy","Cloud Governance","Cloud Adoption Framework","Digital Transformation","Pathfinder Phase"],"content":"In the Align stage of the Cloud Center of Excellence journey, the vision becomes an actionable strategy. Learn how to structure workshops, resolve impediments, and secure executive buy-in to launch your CCoE successfully.","date":"2025-04-07","objectID":"/posts/2025/building-a-cloud-center-of-excellence-the-align-stage/","tags":["Cloud Center of Excellence","CCoE","Cloud Strategy","Cloud Governance","Cloud Adoption","Organizational Change","Team Topologies","Azure","Enablement","Excellence","Cloud","Microsoft"],"title":"üß©üö¶ Building a Cloud Center of Excellence: The Align Stage ‚öôÔ∏èüîß","uri":"/posts/2025/building-a-cloud-center-of-excellence-the-align-stage/"},{"categories":["Cloud Center of Excellence","Cloud Strategy","Cloud Governance","Cloud Adoption Framework","Digital Transformation","Pathfinder Phase"],"content":" ‚ÄúYou can‚Äôt move fast if no one‚Äôs steering in the same direction.‚Äù So far in our Building a Cloud Center of Excellence series, we‚Äôve explored the big picture: In The Pathfinder Journey, we set the stage for the transformation ahead, breaking down the roadmap to cloud maturity. In The Envision Stage, we defined the why behind the CCoE, clarifying the vision, building a common frame of reference, and securing leadership buy-in. Now it‚Äôs time to shift gears. The vision is clear, but alignment is what turns that vision into an actionable strategy. This is where planning gets real, responsibilities take shape, and the CCoE begins to operate as more than just an idea. Welcome to the Align stage. The vision honeymoon is over. Now it's time to get real and create the blueprint. üß≠ What Is the Align Stage?\rThis is where the CCoE goes from being someone else‚Äôs idea (best practices) to becoming part of your own organization. The Align stage is about bridging the gap between the high-level vision and the everyday truth of the teams, budgets, capabilities, and executive sponsorship. This is where you start to take responsibility of the CCoE and own the pathfinder journey to make it a reality. Think of this phase as tuning the orchestra after choosing the music. Everyone needs to know their part and be in tune before the performance begins. The team needs to comprehend the sheets of music and own their specific parts, to make up a whole. You start to do dry runs of the songs to validate everyone understands their role. You appoint a director for the orchestra. üõ†Ô∏è Organize the Alignment Workshops\rThe heartbeat of this stage? Alignment workshops. These sessions bring together the people, processes, and priorities needed to move forward. You establish common ground. Here‚Äôs how we structure them: 1. Fill the CCoE Teams and Assign Team Representatives\rStart by finalizing who‚Äôs involved. Each team in the envisioned CCoE gets a dedicated representative. You already defined the core team structure during the Envision stage. As a quick refresher, check out the next image. Cloud Center of Excellence Model\rThe goal here is to identify actual individuals to represent the six essential role types: Innovators, Enablers, Visionaries, Controllers, Designers, and Owners. Innovators are the hands-on experts: solution engineers, team leads, technical fellows, and chief engineers. They love solving problems and aren‚Äôt afraid to challenge the status quo. Enablers help others succeed. These are Cloud Architects, Cloud Engineers, and DevOps Engineers who will help define and promote the new Cloud Operating Model across your organization. Visionaries are strategic stakeholders who define the direction of the Cloud journey. Think CIO‚Äôs, CTO‚Äôs, enterprise architects, regulatory advisors, and CISOs. They ensure alignment with business goals and long-term value, and represent the executive vision of the organization. Controllers ensure tactical alignment. These might include program managers, compliance officers, financial controllers, security officers, and department heads who shape governance, budgeting, and compliance strategies. Designers are the blueprint builders: Platform-, Solution-, Data-, and Network Architects who design the infrastructure and services that bring your Cloud strategy to life. Owners hold overall accountability for the CCoE‚Äôs success. They act as the bridge between the CCoE and the Cloud Platform Team. Their key responsibility is to ensure the backlog produced by the CCoE is well refined, prioritized, and actionable for iterative implementation. They curate the products \u0026 services catalog through the backlog. There must always be a lead among the owners, who acts as a portfolio manager for the entire CCoE. When multiple inividuals have been identified to represent a role type they become a team. During the Pathfinder phase each teams can be represented by a single lead (start small). A lead innovater, enabler, visionary, controller, designer,","date":"2025-04-07","objectID":"/posts/2025/building-a-cloud-center-of-excellence-the-align-stage/:0:0","tags":["Cloud Center of Excellence","CCoE","Cloud Strategy","Cloud Governance","Cloud Adoption","Organizational Change","Team Topologies","Azure","Enablement","Excellence","Cloud","Microsoft"],"title":"üß©üö¶ Building a Cloud Center of Excellence: The Align Stage ‚öôÔ∏èüîß","uri":"/posts/2025/building-a-cloud-center-of-excellence-the-align-stage/"},{"categories":["Cloud Center of Excellence","Cloud Strategy","Cloud Governance","Cloud Adoption Framework","Digital Transformation","Pathfinder Phase"],"content":"Kickstart your CCoE journey with the Envision Stage. Learn how to assemble your Pathfinder team, run effective workshops, and align leadership using industry best practices to build a high-impact Cloud Center of Excellence.","date":"2025-04-01","objectID":"/posts/2025/building-a-cloud-center-of-excellence-the-envision-stage/","tags":["Cloud Center of Excellence (CCoE)","Cloud Adoption","Cloud Strategy","Pathfinder Phase","Cloud Governance","Digital Transformation","Cloud Best Practices","Cloud Operating Model"],"title":"üåêüèóÔ∏è Building a Cloud Center of Excellence: The Envision Stage üí°üöÄ","uri":"/posts/2025/building-a-cloud-center-of-excellence-the-envision-stage/"},{"categories":["Cloud Center of Excellence","Cloud Strategy","Cloud Governance","Cloud Adoption Framework","Digital Transformation","Pathfinder Phase"],"content":"In our Pathfinder journey to establish a Cloud Center of Excellence (CCoE), we begin with the Envision Stage‚Äîthe first of the stages. This stage creates a common frame of reference for your organization to define the CCoE‚Äôs purpose, structure, and alignment within the organization. If you haven‚Äôt read the introduction to this series, check out the first post: Building a Cloud Center of Excellence: The Pathfinder Journey. Assembling the Pathfinder Team\rBefore the actual CCoE is established, a Pathfinder Team is formed. This team is responsible \u0026 accountable for implementing the Pathfinder Roadmap, a framework designed to guide organizations through cloud adoption and governance. It‚Äôs important to note that the Pathfinder Team is separate from the eventual CCoE members‚Äîit plays a critical role in shaping the foundational elements of the CCoE. It‚Äôs important to note that this is not the definitive CCoE team but rather a group that steers its initial formation. It is often the case that a subset of individuals of the Pathfinder Team will eventually become part of the CCoE, but this is not a requirement. The Pathfinder Team should be composed of individuals with diverse skills and backgrounds. Those skills are aimed at establishing the desired target state, while a CCoE (target state) team member will put more emphasis on operating the established CCoE. This team will: Research and introduce best practices related to CCoE frameworks. Define the vision and objectives of the CCoE. Identify key stakeholders and sponsors. Establish a framework for governance and collaboration. Output initial backlogs for the CCoE team to get started in sprints or iterations. Facilitate transition of the Pathfinder team to a permanent CCoE team. Organizing the Envision Workshops\rThe Envision Stage is structured around a series of workshops that help define the scope, objectives, and structure of any CCoE according to industry-standard best practices. These workshops cover key topics, including: 1. Setting the Stage: Expected Outcomes of the Pathfinder Phase\rDefining the CCoE‚Äôs Role: Establishing an Advisory, Functional, and Prescriptive CCoE model that provides governance, best practices, and implementation guidance. Clarifying the Pathfinder Phase: Ensuring alignment with organizational goals and setting expectations for Cloud adoption, security, and operations. 2. Organizational Structure and CCoE Positioning\rMapping the CCoE within the Organization according to industry standards: Understanding where the CCoE fits in the larger enterprise landscape. Defining Key Interactions: Establishing relationships with business units, IT teams, security teams, and governance bodies. Articulating a Clear Vision: Aligning with industry standards to define a robust mission statement that guides the Pathfinder team decision-making and Cloud strategy. Industry Standard Mission Statement\rThe mission of the Cloud Center of Excellence (CCoE) is to drive strategic Cloud adoption, governance, and innovation across the organization. By establishing best practices, fostering collaboration, and ensuring alignment with business objectives, the CCoE aims to maximize the value of Cloud investments, enhance operational efficiency, and accelerate digital transformation. The CCoE will serve as a central hub for Cloud expertise, providing guidance, tools, and support to enable successful Cloud initiatives and empower teams to leverage Cloud technologies effectively.\r3. The Three Pillars of a CCoE (Gartner Framework)\rA successful CCoE stands on three foundational pillars: Three Pillars of CCoE\rGovernance Capabilities Cloud Strategy and Governance: Enterprise Architecture, Cloud Strategy Development. Financial and Resource Management: Cost control, supplier management, capacity planning. Cloud Security and Compliance: Risk management, data governance, policy enforcement. Vendor and Partner Management: Managing cloud service providers, contracts, and procurement. Brokerage Capabilitie","date":"2025-04-01","objectID":"/posts/2025/building-a-cloud-center-of-excellence-the-envision-stage/:0:0","tags":["Cloud Center of Excellence (CCoE)","Cloud Adoption","Cloud Strategy","Pathfinder Phase","Cloud Governance","Digital Transformation","Cloud Best Practices","Cloud Operating Model"],"title":"üåêüèóÔ∏è Building a Cloud Center of Excellence: The Envision Stage üí°üöÄ","uri":"/posts/2025/building-a-cloud-center-of-excellence-the-envision-stage/"},{"categories":["Cloud Center of Excellence","Cloud Strategy","Cloud Governance","Cloud Adoption Framework","Digital Transformation","Pathfinder Phase"],"content":"Discover the essential steps to building a Cloud Center of Excellence (CCoE) with our comprehensive guide. Learn about the Pathfinder Phase, including Envision, Align, Launch, and Scale stages, and gain actionable insights to drive innovation and efficiency in your cloud journey.","date":"2025-03-25","objectID":"/posts/2025/building-a-cloud-center-of-excellence-the-pathfinder-journey/","tags":["Cloud Center of Excellence (CCoE)","Cloud Adoption","Cloud Strategy","Pathfinder Phase","Cloud Governance","Digital Transformation","Cloud Best Practices","Cloud Operating Model"],"title":"üåêüíº Building a Cloud Center of Excellence: The Pathfinder Journey üåüüöÄ","uri":"/posts/2025/building-a-cloud-center-of-excellence-the-pathfinder-journey/"},{"categories":["Cloud Center of Excellence","Cloud Strategy","Cloud Governance","Cloud Adoption Framework","Digital Transformation","Pathfinder Phase"],"content":"The concept of a Cloud Center of Excellence (CCoE) is well-documented, and many organizations recognize its added value in driving innovation and operational efficiency. However, while there is a wealth of information on what a CCoE is, practical guidance on how to effectively implement one is often lacking, leaving organizations with more questions than answers. Even comprehensive resources like Microsoft‚Äôs Cloud Adoption Framework primarily focus on the functions of a CCoE in its end-state without delving into the intricacies of its establishment. This blog series aims to fill that gap by providing a detailed roadmap for creating a successful Cloud Center of Excellence (CCoE). Drawing from (mine \u0026 Mike Beerman‚Äôs) personal experiences and insights, this series will guide organizations through the various phases of establishing a CCoE, offering actionable steps and best practices to navigate this complex journey. The Pathfinder Phase, also known as Quartering, is where this journey begins. It serves as an exploratory phase that shapes the foundation of a successful CCoE and culminates in scaling the CCoE to drive broader organizational impact. We envision this phase to consist of four key stages: Envision, Align, Launch, and Scale. Each stage of the Pathfinder Phase serves a distinct purpose, and this post will provide an overview of these stages, setting the stage for more detailed breakdowns in subsequent posts. Whether you‚Äôre just starting your CCoE journey or looking to optimize an existing framework, this series will provide valuable insights to help you succeed. The process of establishing a CCoE\rThe Concept of Quartering (Pathfinding)\rThe role of a pathfinder in establishing a CCoE can be likened to the concept of a Quartermaster, a unique profession loosely comparable to the civilian role of a ‚ÄúChief of Staff‚Äù. Historically (dating back to the 15th century in Europe), a ‚ÄúQuartermaster‚Äù (Kwartiermaker in Dutch, or Kwartier-meester in early 15th century Dutch naval terms) in the military was responsible for preparing new military encampments, including strategic placement and provisioning of logistical supplies. This ensured that everything was in place for the troops to settle in efficiently and safe from hostile forces. In the context of a CCoE, pathfinders act as pioneers or trailblazers who lay the groundwork for something new within or between organizations. They work closely with stakeholders to shape this new initiative until it can be transitioned into a new organizational unit. This process involves continuous adaptation and preparation, much like the role of a Quartermaster. This ensures that the foundation is solid before scaling the initiative for production purposes. Envision Stage\rEstablish the foundational vision and framework for the CCoE. Assemble the Pathfinder Team: Identify key stakeholders and early adopters to form a team that will champion the CCoE. Set the Stage: Define the mission, expected outcomes, and long-term goals of the Pathfinder stage. Organizational Structure: Establish reporting lines, team roles, and collaboration models for the Pathfinder stage. Industry-Standard CCoE Mission Statement: Present the industry-standard CCoE mission statement to the team, ensuring they understand and acknowledge its purpose. Three Pillars of a CCoE by Gartner: Align on Governance, Brokerage, and Community capabilities and determine their practical implications. Enhanced Operational Management Pyramid: Align CCoE objectives with business strategies at all levels. Establish the CCoE Teams: Detail the core teams within the CCoE and their primary functions aligned with Team Topologies. Explain the CCoE Roles: Define responsibilities for key roles, including Cloud Architects, Engineers, and Governance Leads. Establish Foundational References and Best Practices: Define key academic and industry sources that underpin the CCoE vision, alongside guiding principles, market standards, and real-world case studies. E","date":"2025-03-25","objectID":"/posts/2025/building-a-cloud-center-of-excellence-the-pathfinder-journey/:0:0","tags":["Cloud Center of Excellence (CCoE)","Cloud Adoption","Cloud Strategy","Pathfinder Phase","Cloud Governance","Digital Transformation","Cloud Best Practices","Cloud Operating Model"],"title":"üåêüíº Building a Cloud Center of Excellence: The Pathfinder Journey üåüüöÄ","uri":"/posts/2025/building-a-cloud-center-of-excellence-the-pathfinder-journey/"},{"categories":["Azure DevOps Fundamentals"],"content":"Using Azure Boards","date":"2025-02-27","objectID":"/posts/2025/azure-devops-fundamentals-understanding-work-items/","tags":["Azure DevOps","Fundamentals","Azure Boards","Backlog","Work Items"],"title":"üìö Azure DevOps Fundamentals: Understanding Work Items in Azure Boards üñâÔ∏èüéØ","uri":"/posts/2025/azure-devops-fundamentals-understanding-work-items/"},{"categories":["Azure DevOps Fundamentals"],"content":"This is the 6th post in the category Azure DevOps Fundamentals of the blog post series on working with Azure DevOps. In this post, I will break down Work Items in Azure Boards. Work Items are the foundation of tracking work in Azure DevOps, and understanding them is crucial for effectively managing projects. We will explore: The different types of Work Items. How Work Items tie into the Work Item Process. The hierarchy of Work Items. Where and how Work Items can be created. Ownership and refinement responsibilities for each Work Item level. Additional useful Work Item features. Let‚Äôs dive in! What Are Work Items in Azure DevOps?\rA Work Item in Azure DevOps represents any unit of work that needs to be tracked. Whether it‚Äôs a feature request, a task, a bug, or an overarching epic, Work Items help teams stay organized and aligned with their goals. Work Items exist within a Work Item Process, which dictates how they behave and relate to each other. Work Item Types and Processes\rAzure DevOps supports different Work Item types based on the process used in the project. The available Work Item types depend on the chosen Work Item Process: Work Item Processes in Azure DevOps\rAzure DevOps provides four default Work Item Processes: Basic ‚Äì A simple process with only three Work Item types: Epics, Issues, and Tasks. Agile ‚Äì Designed for Agile methodologies, including Epics, Features, User Stories, and Tasks. Scrum ‚Äì A process tailored for Scrum teams, featuring Epics, Features, Product Backlog Items (PBIs), and Tasks. CMMI ‚Äì For Capability Maturity Model Integration (CMMI) workflows, including Epics, Features, Requirements, Change Requests, and Tasks. Each process determines the structure and relationships between Work Items. For example, in Scrum, a Product Backlog Item (PBI) is used instead of a User Story, which is the equivalent term in the Agile process. Work Item Hierarchy and Responsibilities\rWork Items follow a hierarchy that organizes work into manageable levels. Understanding this structure is key to effectively managing Azure DevOps projects. Level Agile Process Scrum Process Suggested Duration Owned By 1. Epic Epic Epic Several months Product Management 2. Feature Feature Feature 4-8 weeks Product Owner 3. Story User Story Product Backlog Item (PBI) \u003c 2 weeks Scrum Team 4. Task Task Task \u003c 1 day Developers (Bug) Bug Bug Varies Developers/Testers Explanation of Ownership and Refinement Responsibilities\rEpics: Owned by Product Management, refined collaboratively with Architects to align with strategic goals. Features: Managed by the Product Owner with input from Architects to ensure alignment with technical and business priorities. User Stories/PBIs: Created and refined by the Product Owner with support from the Development Team to ensure feasibility and clarity. Tasks: Owned by Developers and refined collaboratively by the Development Team during Sprint Planning or daily stand-ups. Bugs: Managed by Developers and Testers, refinement depends on severity and impact. Key Fields for Work Items\rSeveral key fields exist across different Work Item types in Azure Boards. Understanding their purpose and applicability helps teams prioritize, estimate, and track work effectively. Below is an overview of important fields and how they apply to each Work Item type. Priority\rAvailable in: Epics, Features, PBIs, User Stories, Bugs, and Tasks. Usage: Indicates the importance of the Work Item relative to others. Higher priority items should be addressed first. Example: A Feature with a priority of 1 should be developed before a priority 3 Feature. Effort\rAvailable in: Features, PBIs, User Stories, and Tasks. Usage: Represents the estimated amount of work required to complete the Work Item. Helps in sprint and capacity planning. Example: A User Story estimated at 5 effort points indicates it requires more work than one estimated at 2 points. Business Value\rAvailable in: Epics, Features, PBIs, and User Stories. Usage: Helps determine the value deli","date":"2025-02-27","objectID":"/posts/2025/azure-devops-fundamentals-understanding-work-items/:0:0","tags":["Azure DevOps","Fundamentals","Azure Boards","Backlog","Work Items"],"title":"üìö Azure DevOps Fundamentals: Understanding Work Items in Azure Boards üñâÔ∏èüéØ","uri":"/posts/2025/azure-devops-fundamentals-understanding-work-items/"},{"categories":["Azure DevOps Governance","Automation","Cloud Compliance","PowerShell"],"content":"Learn how to automate Azure DevOps audit stream configurations using PowerShell to ensure compliance, governance, and security. This guide highlights the importance of storing logs securely in Log Analytics Workspaces, provides an in-depth PowerShell script, and explains how to maintain consistent audit log practices across your organization.","date":"2025-01-21","objectID":"/posts/2025/automating-azure-devops-auditing-configure-streams-with-powershell/","tags":["Azure DevOps","Bicep","PowerShell","Log Analytics","Cloud Governance"],"title":"üìä Automating Azure DevOps Auditing: Configure Streams with PowerShell","uri":"/posts/2025/automating-azure-devops-auditing-configure-streams-with-powershell/"},{"categories":["Azure DevOps Governance","Automation","Cloud Compliance","PowerShell"],"content":"Introduction\rManaging compliance and governance in the cloud can be challenging, especially when it comes to an often overlooked yet critical system: your Azure DevOps organization. Ensuring proper auditing with securely stored logs in a centralized location is vital. Audit streams address this need by enabling you to send audit logs to a Log Analytics Workspace for detailed analysis and monitoring. In this blog post, we‚Äôll explore how to automate the configuration of Azure DevOps audit streams using PowerShell. We‚Äôll focus on using a fully working script that not only automates the setup process but also ensures drift control. This guarantees that audit logs are consistently sent to your desired Log Analytics Workspace without the need for manual intervention. Let‚Äôs dive in! Why Audit Streams Matter\rAudit streams in Azure DevOps provide a mechanism to export auditing events to external storage, such as Log Analytics. This enables organizations to: Monitor activities and changes in Azure DevOps for compliance. Detect anomalies and potential security risks. Store logs in a central, queryable location for analysis and reporting. Storing logs from an Azure DevOps organization is vital because: Compliance and Governance: Many organizations operate in regulated industries where maintaining a detailed audit trail of changes and actions is mandatory to meet compliance standards. Security Monitoring: Azure DevOps contains the blueprint of your applications‚Äîyour organization‚Äôs code. It also includes highly privileged identities used to deploy resources in your cloud environment. Monitoring logs helps detect unusual or unauthorized activities, such as unexpected changes to repositories or misuse of these privileges. Troubleshooting and Forensics: Logs provide a historical record that is invaluable when investigating incidents, understanding deployment failures, or identifying the root cause of other issues in your DevOps pipeline. Accountability: Logs enable you to track ‚Äúwho did what and when,‚Äù promoting transparency and accountability within your teams. This is particularly important in collaborative environments where multiple users interact with critical resources. Centralized Analysis and Reporting: Exporting logs to systems like Log Analytics enables advanced querying, visualization, and integration with monitoring or incident management tools. This centralized approach ensures efficient analysis and reporting, helping maintain operational visibility across your Azure DevOps environment. By securely storing and analyzing these logs, organizations can safeguard their intellectual property, maintain a strong security posture, and ensure operational continuity. The PowerShell Script: New-AdoAuditStream\rBelow is a PowerShell script designed to automate the configuration of Azure DevOps audit streams. It does the following: Ensures the specified Log Analytics Workspace exists and retrieves its properties. Queries existing audit streams in the Azure DevOps organization. Deletes any rogue streams not aligned with the desired configuration. Verifies that the target audit stream is enabled and correctly configured. If it isn‚Äôt, the script updates or creates the stream as needed. Latest version of the full script as show in the above snippet is available in the simply-scripted repo of the The Cloud Explorers GitHub How to Use the Script\rSet Up Authentication To interact with the Azure DevOps REST API, you‚Äôll need an authentication token. Set this up as follows: Or you can use my New-AdoAuthenticationToken function available in the simply-scripted repo of the The Cloud Explorers GitHub Define Parameters Replace the placeholders in the parameter block with your organization name, Log Analytics Workspace name, and resource group. Run the Script Execute the script with the provided arguments: Benefits of Automation\rAutomating the configuration of audit streams offers several advantages: Consistency: Ensures your Azure DevOps organization adhere","date":"2025-01-21","objectID":"/posts/2025/automating-azure-devops-auditing-configure-streams-with-powershell/:0:0","tags":["Azure DevOps","Bicep","PowerShell","Log Analytics","Cloud Governance"],"title":"üìä Automating Azure DevOps Auditing: Configure Streams with PowerShell","uri":"/posts/2025/automating-azure-devops-auditing-configure-streams-with-powershell/"},{"categories":["Azure Governance","Azure Bicep","How to","Tenant Landing Zone"],"content":"Streamline your Azure environment with a reusable Bicep solution that deploys hierarchical management groups based on the Cloud Adoption Framework (CAF). Includes step-by-step guidance and code examples.","date":"2025-01-07","objectID":"/posts/2025/simplify-azure-management-group-setup-using-bicep-vending/","tags":["Azure","Bicep","Cloud Adoption Framework","Management Groups"],"title":"Simplify Azure Management Group Setup using Bicep Vending","uri":"/posts/2025/simplify-azure-management-group-setup-using-bicep-vending/"},{"categories":["Azure Governance","Azure Bicep","How to","Tenant Landing Zone"],"content":"Understanding the Need for Management Group Automation\rManaging Azure subscriptions at scale can become challenging. Management groups provide a powerful way to organize and govern resources, but manually configuring them introduces inefficiencies and risks. A common pitfall is hardcoding management group structures into deployment code, which reduces flexibility and can compromise governance. This post introduces Management Group Vending, a reusable Bicep solution inspired by Microsoft‚Äôs Subscription Vending. This solution enables you to deploy any management group hierarchy, considering Azure‚Äôs limitations while adhering to Cloud Adoption Framework (CAF) principles. Design for Vending\rAs of writing this post the maximum depth limit for management groups is 6 levels. This limitation is very helpful it means that out scope is limited to 6 levels. So we can slice any kind of Management Group structure into a maximum of 6 levels or as I like to call them tiers. As an practice example lets use this approach on the management group structure as show in the Azure landing zone conceptual architecture part of the Cloud Adoption Framework (CAF). The CAF conceptual architecture serves as a solid foundation for designing management group hierarchies, as it reflects industry best practices for organizing Azure resources. Slicing this management groups structure into tiers would have the following structure: Tier 0: Contoso (Corporate Root) Tier 1: Platform, Landing Zones, Decommissioned, Sandbox Tier 2: Management, Identity, Connectivity, Corp, Online Tier 3: Unused Tier 4: Unused Tier 5: Unused And visualized as follows: Management Group Vending applied to Microsoft Enterprise Landing Zone\rWhy start at zero?\rThe first tier represents the corporate root management group, which is a single entity. Starting at zero emphasizes its uniqueness compared to subsequent tiers, which are collections. This approach improves both readability and maintainability.\rNow that the tier model is clear lets compose the solution which implements this model. First we have to have a solution bicep file. This is the main execution file which will be executed to deploy the vending solution. This file will contain the logic to deploy the management groups in the correct order and depth. Next we need a bicep resource module. This module will contain the logic to deploy a single management group, hence the resource module. This module is called by the solution bicep file for each management group in a tier. Finally we need a parameter file which provides the collections of management groups to be deployed. On the back of an envelope design would looks something like this: Management Group Vending Structure\rBuild the Management Group Vending Bicep Solution\rThe solution is composed of three main components: Resource Module\rThe resource module is the core building block of the solution. It leverages the Microsoft.Management/managementGroups@2021-04-01 namespace to create individual management groups. The key aspect here is the targetScope = 'managementGroup' property, which allows the deployment of management groups within the correct scope. This module handles the creation of a single management group, including setting its ID, display name, and parent ID. This modular design ensures reusability and keeps the logic for creating management groups isolated. Below is an example of a simple resource module: Solution Bicep File\rThe solution Bicep file acts as the orchestrator. It consumes the resource module to deploy the management group hierarchy tier by tier. Each tier is represented as a collection of management groups, except for Tier 0, which is the single corporate root management group. Key highlights of the solution file include: Tier Dependencies: Each tier depends on the successful creation of its parent tier. This ensures management groups are deployed in the correct order and hierarchy. Default Management Group: Optionally, you can set a default management gr","date":"2025-01-07","objectID":"/posts/2025/simplify-azure-management-group-setup-using-bicep-vending/:0:0","tags":["Azure","Bicep","Cloud Adoption Framework","Management Groups"],"title":"Simplify Azure Management Group Setup using Bicep Vending","uri":"/posts/2025/simplify-azure-management-group-setup-using-bicep-vending/"},{"categories":["Azure","Azure Landing Zones","Cloud Infrastructure"],"content":"Dive into the world of Azure Landing Zones and discover the differences between Cloud, Platform Application and Tenant zones. Understand how each plays a vital role in building a scalable and secure cloud environment.","date":"2024-12-16","objectID":"/posts/2024/decoding-microsoft-azure-landing-zones-explained/","tags":["Azure Landing Zones","Platform Landing Zone","Application Landing Zone","Tenant Landing Zone","Microsoft Azure","Cloud Infrastructure"],"title":" üîç Decoding Microsoft Azure: Landing Zones Explained üõ¨","uri":"/posts/2024/decoding-microsoft-azure-landing-zones-explained/"},{"categories":["Azure","Azure Landing Zones","Cloud Infrastructure"],"content":"In my previous Decoding Microsoft Azure post, üîç Decoding Microsoft Azure: Understanding Platform, Landing Zones, Workloads, and Utilities ‚öôÔ∏è, I introduced the house analogy to conceptually explain Microsoft Azure. Part of this analogy covered the Application Landing Zone and the Platform Landing Zone. In this post, I‚Äôll take a deeper dive into Landing Zones. We‚Äôll explore what they are, how they‚Äôre defined by Microsoft, and how I believe we can improve upon the concept. By the end, you‚Äôll have a clearer understanding of Azure Landing Zones and how they fit into a broader cloud adoption strategy. What is an Azure Landing Zone? ‚Äì Microsoft‚Äôs Definition\rThere‚Äôs no better place to start than the official documentation: What is an Azure landing zone?. Microsoft defines an Azure Landing Zone as: An Azure landing zone is an environment that follows key design principles across eight design areas. These design principles accommodate all application portfolios and enable application migration, modernization, and innovation at scale. The concept includes Platform Landing Zones and Application Landing Zones: Platform Landing Zone: A platform landing zone is a subscription that provides shared services (identity, connectivity, management) to applications in application landing zones. Application Landing Zone: An application landing zone is a subscription for hosting an application. The conceptual architecture of this Azure Landing Zone is shown in the following figure: Azure landing zone conceptual architecture\rConfused? Let‚Äôs put this into perspective. Landing Zones in Perspective\rMicrosoft‚Äôs documentation describes the overall architecture as an Azure Landing Zone. However, its depiction extends beyond Azure itself, including elements like Entra ID and automation tooling (e.g., Azure DevOps, GitHub). To better reflect its scope, I propose calling this broader concept an Enterprise Cloud Landing Zone. This term accounts for all components depicted while staying conceptually aligned. Here‚Äôs a simplified view: Simplified - Enterprise Cloud Landing Zone\rWithin this Enterprise Cloud Landing Zone, we see smaller Landing Zones, such as: Platform Landing Zones Application Landing Zones However, not all depicted components are part of two these categories. As shown in the following figure: Azure Landing Zone in perspective\rTo clarify the Landing Zones within the Enterprise Cloud Landing Zone, I suggest viewing them as patterns, similar to Cloud Design Patterns. These patterns help organize the conceptual architecture and provide a consistent framework for understanding. The pattern naming compliments the Landing Zone Archetypes explained in the Microsoft Azure Landing Zone documentation. Once we establish this perspective, we can introduce the Tenant Landing Zone. This addition expands the concept to encompass all essential components, completing the picture as shown below: Landing Zone Patterns\rPlotting Patterns to Azure Landing Zones\rBy overlaying the patterns onto Microsoft‚Äôs conceptual architecture, the picture becomes complete and unambiguous: Landing Zone Patterns Plotted on Azure conceptual architecture\rFrom Patterns to Implementation\rUnderstanding Landing Zones conceptually is just the beginning. Implementation involves aligning these patterns with organizational goals, technical requirements, and governance models. Here‚Äôs how each pattern contributes: Platform Landing Zone: Provides foundational services, such as connectivity (hub-spoke or mesh), monitoring, and identity management. Application Landing Zone: Creates isolated environments for workloads, tailored to specific application needs. Tenant Landing Zone: Ensures proper governance and segmentation for managing Entra ID tenants and automation tooling like Azure DevOps* or GitHub. When implemented effectively, these patterns combined with the archetypes form a scalable and secure foundation for enterprise cloud adoption. The challenge lies in tailoring them to your organization‚Äôs u","date":"2024-12-16","objectID":"/posts/2024/decoding-microsoft-azure-landing-zones-explained/:0:0","tags":["Azure Landing Zones","Platform Landing Zone","Application Landing Zone","Tenant Landing Zone","Microsoft Azure","Cloud Infrastructure"],"title":" üîç Decoding Microsoft Azure: Landing Zones Explained üõ¨","uri":"/posts/2024/decoding-microsoft-azure-landing-zones-explained/"},{"categories":["Azure","Azure Sentinel","Azure Security","How to"],"content":"Learn how to deploy Azure Sentinel to an existing Log Analytics workspace using Bicep for a streamlined and efficient setup.","date":"2024-11-25","objectID":"/posts/2024/you-had-me-at-bicep-deploying-microsoft-sentinel-made-easy/","tags":["Azure Sentinel","Bicep","Log Analytics","Microsoft Azure","Cloud Security"],"title":"üõ°Ô∏è You Had Me at Bicep: Deploying Microsoft Sentinel Made Easy üí™","uri":"/posts/2024/you-had-me-at-bicep-deploying-microsoft-sentinel-made-easy/"},{"categories":["Azure","Azure Sentinel","Azure Security","How to"],"content":"In this post, I‚Äôll demonstrate how to quickly and easily deploy Microsoft Sentinel to an existing Log Analytics workspace using Bicep. This step-by-step guide simplifies the process, showing that deploying Microsoft Sentinel doesn‚Äôt have to be complicated‚Äîwhether you‚Äôre an experienced cloud professional or new to Azure. What is Microsoft Sentinel?\rIf you‚Äôre new to Microsoft Sentinel, here‚Äôs a brief introduction. Feel free to skip this section if you‚Äôre already familiar with it. Microsoft Sentinel is a cloud-native Security Information and Event Management (SIEM) solution that provides intelligent security analytics across your entire enterprise. It helps organizations detect, investigate, and respond to security threats with real-time insights and advanced threat intelligence. By integrating seamlessly with Azure, Microsoft Sentinel offers a scalable platform to monitor, analyze, and protect data across cloud, on-premises, and hybrid environments. For more details, check out the official Microsoft Sentinel documentation. Deploying Microsoft Sentinel using Bicep\r1. Gathering Prerequisites and Creating Parameters\rBefore we start, we need to prepare some input. First, we need the name of the Log Analytics workspace where we want to deploy Microsoft Sentinel. The goal of this post is to maximize modularity and reusability, so we‚Äôll assume that the Log Analytics workspace has already been created. If you need help creating a Log Analytics workspace, check out the official documentation. For the existing workspace, we need the following information: Log Analytics Workspace Name: The name of the Log Analytics workspace where we want to deploy Microsoft Sentinel. Resource Location: The Azure region where the Log Analytics workspace is located and where we want to deploy the Sentinel solution. With this info ready, we can create the parameters as shown in the following snippet. 2. Get the Log Analytics Workspace Resource\rNext, let‚Äôs retrieve the Log Analytics workspace resource, as shown in the following snippet. 3. Deploying the Microsoft Sentinel Solution\rNow, let‚Äôs deploy the Microsoft Sentinel solution to the Log Analytics workspace, as shown in the following snippet. It‚Äôs important to note that we are deploying a gallery solution here. This solution is a prerequisite for onboarding the Log Analytics workspace to Microsoft Sentinel, which we will do in the next step. Simply deploying this resource won‚Äôt be enough for a successful Microsoft Sentinel setup. Info\rThe solution is defined using semi-hardcoded values. Deviating from these values will cause the deployment to fail. This is fundamentally different from how other resource deployments are done in Azure. The name must be defined as SecurityInsights(\u003cYourSentinelWorkspaceName\u003e) The plan name must match exactly as SecurityInsights(\u003cYourSentinelWorkspaceName\u003e) The publisher must be hardcoded to Microsoft. 4. Onboard the Log Analytics Workspace to Sentinel\rFinally, we need to onboard the Log Analytics workspace to Sentinel. This is done by calling the Microsoft.OperationsManagement/solutions/Onboarding resource provider, as shown in the following snippet. Without this step, you can view the Log Analytics workspace in Microsoft Sentinel, but the portal will throw errors when you try to interact with various features. Info\rThe name must be hardcoded to default. Any other value will cause the deployment to fail.\rWrapping up\rAnd that‚Äôs all folks! We have successfully deployed Microsoft Sentinel to an existing Log Analytics workspace using Bicep. I hope you found this post educational and easy to follow. If you‚Äôre interested in the reference material used in this post, please visit the following links: Microsoft Sentinel documentation What is Microsoft Sentinel? Microsoft.OperationsManagement solutions Create a Log Analytics workspace As always, a big thanks for reading! If you liked this post, don‚Äôt be shy‚Äîcheck out my other posts. ","date":"2024-11-25","objectID":"/posts/2024/you-had-me-at-bicep-deploying-microsoft-sentinel-made-easy/:0:0","tags":["Azure Sentinel","Bicep","Log Analytics","Microsoft Azure","Cloud Security"],"title":"üõ°Ô∏è You Had Me at Bicep: Deploying Microsoft Sentinel Made Easy üí™","uri":"/posts/2024/you-had-me-at-bicep-deploying-microsoft-sentinel-made-easy/"},{"categories":["Azure","Azure Bastion","Azure Security","How to","Quickstart"],"content":"Quickstart: Learn to set up Azure Bastion Developer SKU using Infrastructure as Code (IaC) for secure VM access.","date":"2024-11-07","objectID":"/posts/2024/keep-calm-and-use-azure-bastion-developer-sku-for-free/","tags":["Azure Bastion","Developer SKU","Virtual Machines","Secure Access","Microsoft Azure"],"title":"üîí Keep Calm and Use Azure Bastion Developer SKU for Free üíª","uri":"/posts/2024/keep-calm-and-use-azure-bastion-developer-sku-for-free/"},{"categories":["Azure","Azure Bastion","Azure Security","How to","Quickstart"],"content":"In today‚Äôs cloud-driven landscape, security is a shared responsibility for developers and engineers alike. Azure Bastion‚Äôs Developer SKU offers a lightweight way to securely connect to virtual machines‚Äîand did I mention it‚Äôs free? üéâ In this post, we‚Äôll walk through setting up Azure Bastion Developer SKU using Bicep Infrastructure as Code, fostering a ‚Äúsecurity is everyone‚Äôs responsibility‚Äù mindset as we go. What is Azure Bastion?\rLet‚Äôs start with a quick recap for those who are unfamiliar with Azure Bastion. Azure Bastion is a fully managed service provided by Microsoft that enables secure RDP (Remote Desktop Protocol) and SSH (Secure Shell) access to virtual machines (VMs) without exposing them to the public internet. It acts like a jumpbox, providing access through a web browser. This eliminates the need for public IPs, VPNs, or separate client software. Azure Bastion enhances security by protecting VMs within a Virtual Network (VNet) and allowing remote access only through the Azure portal. This significantly reduces the attack surface for VM access, making it a more secure and streamlined solution. Azure Bastion is available in four SKUs: Developer SKU Basic SKU Standard SKU Premium SKU (currently in preview at the time of writing) What makes Azure Bastion Developer SKU so special?\rFor starters, as mentioned, it‚Äôs free to use! Unlike the other SKUs, the Developer SKU is lightweight and specifically designed for developers, engineers, and testers who need secure VM connections but don‚Äôt require advanced Bastion features. A key difference is that the Developer SKU doesn‚Äôt deploy a dedicated host, eliminating the need for an AzureBastionSubnet and a public IP address. Instead, it operates from a shared pool. In the following sections, we‚Äôll explore the unique deployment requirements for this SKU, as well as its pros and cons. Microsoft Azure Bastion Developer SKU architecture. Source Pros and Cons\rPros: Free to use Tailor made for Dev/Test Quick and simple to setup No specific network requirements No public IP address needed Upgradable to a higher SKU, fostering for future growth Cons: No SLA is included Limited to one connection at a time Lacks advanced features like VNet peering, session recording, native client access Lacks scaling capabilities Limited region availability The Developer SKU is considered best for cost-effective development needs, while the other SKUs suit production with advanced requirements. For a full feature comparison list have a look at Azure Bastion SKUs Deploying Azure Bastion Developer SKU using Bicep\rLet‚Äôs dive into the Bicep code! While the Developer SKU has its differences, its infrastructure-as-code implementation is consistent with other SKUs, using the Microsoft.Network/bastionHosts resource type. First, start with basic Bicep module scaffolding: Next, specify the correct SKU name: Unlike other SKUs, the Developer SKU doesn‚Äôt require an AzureBastionSubnet or ipConfigurations, but a reference to an existing virtual network is still needed. Use the existing keyword to correctly reference the virtual network resource: Normally, this is all that‚Äôs needed. However, deploying without a workaround may cause an InternalServerError with An error occurred. message. Thanks to Reddit user Content-Revolution97, we know adding an empty tags property resolves this issue: Finally, assemble everything with parameters and documentation. Your completed module should resemble the following: Wrapping up\rAnd that‚Äôs all folks! I hope you found this post educational. If you are interested in the reference material used to make this post, please visit the following links. Quickstart: Deploy Azure Bastion - Developer SKU Azure Bastion Settings Azure Bastion FAQ Microsoft.Network bastionHosts As always, a big thanks for reading this post. If you liked it, don‚Äôt be shy and have a look at my other posts. ","date":"2024-11-07","objectID":"/posts/2024/keep-calm-and-use-azure-bastion-developer-sku-for-free/:0:0","tags":["Azure Bastion","Developer SKU","Virtual Machines","Secure Access","Microsoft Azure"],"title":"üîí Keep Calm and Use Azure Bastion Developer SKU for Free üíª","uri":"/posts/2024/keep-calm-and-use-azure-bastion-developer-sku-for-free/"},{"categories":["Azure DevOps","Azure DevOps Fundamentals","How to"],"content":"Learn how to effortlessly locate your Azure DevOps Project ID with this quick and easy guide.","date":"2024-08-22","objectID":"/posts/2024/quickly-find-your-azure-devops-project-id/","tags":["Azure DevOps","Project Id","System.TeamProjectId","Hack"],"title":"üöÄ Quickly Find Your Azure DevOps Project Id üîç","uri":"/posts/2024/quickly-find-your-azure-devops-project-id/"},{"categories":["Azure DevOps","Azure DevOps Fundamentals","How to"],"content":"With this post I want to share a lazy and a hacky way for finding the Azure DevOps Project Id of any project in your Azure DevOps Organization. The lazy API query\rIt can feel a bit daunting to start using Azure DevOps API. But what most folks are not aware of is that data retrieval API‚Äôs can be queried using your favorite browser. To get the Project Id we can do this with the projects LIST and GET API‚Äôs. Authentication is required\rNavigating the API‚Äôs is subjected to the same authentication and authorization as regular Azure DevOps browsing. So we can expect a log-in prompt. Also all API‚Äôs are security trimmed to the user permissions in question and will only return data the user in question is authorized to access.\rProjects LIST\rThe list API will return all the available Azure DevOps projects including their project Id‚Äôs. Aure DevOps projects list api https://dev.azure.com/[YOUR_ORGANIZATION_NAME]/_apis/projects?api-version=5.0 Example with demojev Azure DevOps Organization https://dev.azure.com/demojev/_apis/projects?api-version=5.0Projects LIST results\rProjects GET\rThe get api will return information about a single Azure DevOps project, it requires the project name to be included in the uri. Aure DevOps projects get api https://dev.azure.com/[YOUR_ORGANIZATION_NAME]/_apis/projects/The%20Cloud%20Explorers?api-version=5.0 Example with demojev Azure DevOps Organization and The Cloud Explorers project https://dev.azure.com/demojev/_apis/projects/The%20Cloud%20Explorers?api-version=5.0Projects GET results\rThe hacky HTML\rIt turns out that the project id of every project is embedded into the HTML body of Azure DevOps. So the most ‚Äòdirty‚Äô way of getting the project id is to inspect the HTML of the Azure DevOps Organization home page. We cn do this by following the 3 steps below. We need to have Azure DevOps Organization home page and the browsers developer tools (F12 is the common shortcut) open. First lets click on the select an element option in the developers tools Now hover over the desired project block on the Azure DevOps Organization home page Look for the ‚Äòid‚Äô property on the Elements tab of the developer tools HTML hack example\rWrapping up\rAnd that‚Äôs all folks! I hope you found this post educational. If you are interested in the reference material used to make this post, please visit the following links. Projects - Get Projects - List Where can I find System.TeamProjectId for a project in Azure Devops As always, a big thanks for reading this post. If you liked it, don‚Äôt be shy and have a look at my other posts. ","date":"2024-08-22","objectID":"/posts/2024/quickly-find-your-azure-devops-project-id/:0:0","tags":["Azure DevOps","Project Id","System.TeamProjectId","Hack"],"title":"üöÄ Quickly Find Your Azure DevOps Project Id üîç","uri":"/posts/2024/quickly-find-your-azure-devops-project-id/"},{"categories":["Microsoft Azure","Architecture","Cloud Computing"],"content":"Unravel the complexities of Microsoft Azure by exploring key concepts like platforms, landing zones, workloads, and essential utilities.","date":"2024-06-30","objectID":"/posts/2024/decoding-microsoft-azure-understanding-platform-landing-zones-workloads-and-utilities/","tags":["Microsoft Azure","Azure Guide","Landing Zones","Azure Workloads","Cloud Infrastructure","Cloud Adoption Framework"],"title":"üîç Decoding Microsoft Azure: Understanding Platform, Landing Zones, Workloads, and Utilities ‚öôÔ∏è","uri":"/posts/2024/decoding-microsoft-azure-understanding-platform-landing-zones-workloads-and-utilities/"},{"categories":["Microsoft Azure","Architecture","Cloud Computing"],"content":"The task of explaining concepts of the Microsoft Azure Cloud befalls to us Cloud consultants. Usually we have to explain these concepts to stakeholders with varying levels of technical expertise. So using a simple analogy often helps to clarify complex ideas. In this post I want to share my favorite analogy. The house analogy! I am sure that I am not the first to come up with this analogy, however if you like my variation, feel free to use it in your own presentations or discussions. Decoding key components of Microsoft Azure\rLets assume this AI generated house represents Microsoft Azure and start decoding the key components while mapping them to this house. Microsoft Azure - The House\rCloud Platform\rThe platform is the neighborhood where the house is located. It provides the necessary infrastructure, services, and tools that support the house and its residents. The Platform is a foundational layer managed by a dedicated platform team. This layer encompasses the logical structure, core infrastructure, services, and tools that are shared across an organization‚Äôs cloud landscape. The platform is a shared foundation that enables workload teams to focus on application-specific tasks while benefiting from the robust, secure, and efficient environment maintained by the platform team. Platform - The neighborhood\rPlatform / Foundation Utilities\rPlatform utilities are the roads, streets, electricity, and other essential services that support the neighborhood. They are the backbone of the platform, providing the necessary infrastructure and resources to ensure the smooth operation of the neighborhood. Platform / Foundation utilities are services which are piped into each landing zone as a set of utility services. Separating platform utilities from the landing zones maximizes consistency and economy of scale. These utilities also create clear distinctions between platform managed responsibilities and workload level responsibilities. Platform - Roads, streets, electricity, etc‚Ä¶\rLanding Zone\rThe house itself is the landing zone. It is a pre-configured space that is ready to host workloads securely and efficiently. The house is designed to meet the specific needs of the residents, providing a safe and reliable environment for them to live and work. A Landing Zone is a pre-configured enclosure within Microsoft Azure that is designed to host workloads securely and efficiently. It‚Äôs a customizable foundation complete with best practices for security, compliance, and scalability already in place. This means that when the workload team is ready to deploy a workload, the team can do so with the confidence that it‚Äôs in a well-organized, managed space that follows guidelines for a reliable and secure Cloud experience. The Landing Zone includes among others, network configurations, identity services, and compliance policies that support the specific organization needs, making it easy and quick to get up and running while maintaining high standards for security, compliance and governance. Basically a Landing Zone is the blueprint for organizing workloads in Azure. Platform - The house itself\rLanding Zone Utilities\rThe amenities within the house represent the Landing Zone Utilities. These are the essential features and services that enhance the living experience, making the house more functional, comfortable, and efficient. Landing Zone Utilities are a set of integrated services and tools that are pre-configured and available within the Landing Zone. These utilities include monitoring, logging, backup, and disaster recovery services, which are crucial for maintaining the health and performance of workloads. By incorporating these utilities, the Landing Zone ensures that workloads have the necessary support to operate smoothly and securely. By having these utilities in place, organizations can maintain high standards of operational excellence, security, and compliance, ensuring that workloads are well-supported and resilient. The separation of these ut","date":"2024-06-30","objectID":"/posts/2024/decoding-microsoft-azure-understanding-platform-landing-zones-workloads-and-utilities/:0:0","tags":["Microsoft Azure","Azure Guide","Landing Zones","Azure Workloads","Cloud Infrastructure","Cloud Adoption Framework"],"title":"üîç Decoding Microsoft Azure: Understanding Platform, Landing Zones, Workloads, and Utilities ‚öôÔ∏è","uri":"/posts/2024/decoding-microsoft-azure-understanding-platform-landing-zones-workloads-and-utilities/"},{"categories":["Azure DevOps Governance","Automation","Cloud Compliance","PowerShell"],"content":"Hands-on guide of how add compliance to all pipelines within an Azure DevOps organization","date":"2024-06-14","objectID":"/posts/2024/i-am-in-your-pipeline-decorating-it-with-compliance/","tags":["Azure DevOps","Azure","Compliance","Pipelines","Pipeline Decorators","Microsoft Security DevOps extension"],"title":"üöÄ‚ú® I am in your pipeline, decorating it with compliance üõ†Ô∏èüîí","uri":"/posts/2024/i-am-in-your-pipeline-decorating-it-with-compliance/"},{"categories":["Azure DevOps Governance","Automation","Cloud Compliance","PowerShell"],"content":"Back in 2022 I wrote a blog post I am in your pipeline reading all your secrets! about how secrets can be leaked in Azure Pipelines. I think it‚Äôs time to offset that blog post and have a look at how compliance of all pipelines in a single Azure DevOps Project can be achieved. This is done by using a feature called Pipeline Decorators. What are pipeline decorators?\rIn most organizations there are certain required compliance and security policies. For example; to be compliant with corporate policies a static code analysis tool must to be executed on all pipelines before executing the actual pipeline tasks. This is where pipeline decorators come in, pipeline authors don‚Äôt need to remember to add that step. We as Azure DevOps Organization owners create a decorator that automatically injects the step into all pipelines during their runtime. Ensuring on an Azure DevOps Organization level that all pipelines are compliant with our organization‚Äôs policies. If Pipeline Decorators are so awesome why isn‚Äôt everyone using them and why isn‚Äôt this feature widespread in the community? Be honest,‚Ä¶\rBut don‚Äôt worry, we got this! Trough this blog post we will learn in a step by step manner how to create and publish our own pipeline decorator using the Microsoft Security DevOps Task. This free to use task is a collection of static analysis tools aimed at enhancing the code quality, security and compliance of the development lifecycle. So to double down on the earlier given example we are going to implement it as part of our Pipeline Decorator. We got this!\rPipeline Decorators types\rThere are two types of pipeline decorators: Build Decorators Release Decorators The Build Decorators apply to Classic Build Pipelines and to the newer YAML Multistage Pipelines. The Release Decorators, instead, apply ONLY to the Classic Release Pipelines. Since the Classic Release Pipelines are being deprecated, we will focus on the Build Decorators and consider the Release decorators out of scope. Decomposing a pipeline\rTo make sure everyone is on the same page let‚Äôs quickly recap the YAML Multistage Pipelines composition. As show in the following image, a YAML Multistage Pipeline is composed of one or more stages, each stage is composed of one or more jobs (jobs are executed on an agent), a job contains steps and each step is composed of tasks. Pipeline Decomposition\rInjection options\rPipeline decorators inject steps to the beginning, the end of every pipeline job and before and after a certain task. The injection point is defined by the target property in the decorator configuration. Take a closer look!\rThe following targets are supported for YAML Multistage Pipeline: Decorator Injection options\rTarget Description ms.azure-pipelines-agent-job.pre-job-tasks Run before other tasks in a classic build or YAML pipeline. ms.azure-pipelines-agent-job.post-checkout-tasks Run after the last checkout task in a classic build or YAML pipeline. ms.azure-pipelines-agent-job.post-job-tasks Run after other tasks in a classic build or YAML pipeline. ms.azure-pipelines-agent-job.pre-task-tasks Run before specified task in a classic build or YAML pipeline. ms.azure-pipelines-agent-job.post-task-tasks Run after specified task in a classic build or YAML pipeline. Source Lets decorate your pipelines\rFirst lets make sure we are all set by checking the following prerequisites. Get our VS Code or any other code editor ready and create a new folder. Preferable one that is source controlled! Make sure the identity we are using is a member of the Project Collection Administrators in our Azure DevOps Organization. Make sure we have the Microsoft Security DevOps extension installed in our Azure DevOps Organization. Make sure we have Node.js installed on our machines. Install the packaging tool (TFX) by running this command: npm install -g tfx-cli Create the decorator extension\rIn the folder we created as part of the prerequisites initialize a new npm package manifest by running hte following comm","date":"2024-06-14","objectID":"/posts/2024/i-am-in-your-pipeline-decorating-it-with-compliance/:0:0","tags":["Azure DevOps","Azure","Compliance","Pipelines","Pipeline Decorators","Microsoft Security DevOps extension"],"title":"üöÄ‚ú® I am in your pipeline, decorating it with compliance üõ†Ô∏èüîí","uri":"/posts/2024/i-am-in-your-pipeline-decorating-it-with-compliance/"},{"categories":["Exchange Online","Outlook","Microsoft 365","Troubleshooting"],"content":"LOL","date":"2024-04-29","objectID":"/posts/2024/400-bad-request-access-to-outlook-on-the-web-has-been-blocked-by-your-organization/","tags":["Exchange Online","Outlook","Microsoft 365","Troubleshooting"],"title":"üö´ 400 Bad Request. Access to Outlook on the web has been blocked by your organization üõë‚úâÔ∏èüîí","uri":"/posts/2024/400-bad-request-access-to-outlook-on-the-web-has-been-blocked-by-your-organization/"},{"categories":["Exchange Online","Outlook","Microsoft 365","Troubleshooting"],"content":"While this post is outside of my usual topics, I would like to share a solution for an error message which returns just a single search result on Google. I want to share it because it took me some time to figure out what was going on. So in case someone else runs into this issue this post might be helpful. The Issue\rIt all started after the network team updated the network to a new public IP. Right after this change all users started to get the error message 400 Bad Request. QWNjZXNzIHRvIE91dGxvb2sgb24gdGhlIHdlYiBoYXMgYmVlbiBibG9ja2VkIGJ5 IHlvdXIgb3JnYW5pemF0aW9uLiA8QlI+PEJSPkNsaWNrIDxhIGhyZWY9Ii9vd2Ev bG9nb2ZmLm93YSI+aGVyZTwvYT4gdG8gc2lnbiBvdXQu (Base64) when trying to access Outlook on the web (OWA) in Microsoft 365. Outlook Web App - 400 Bad Request\rAnd in case you are wondering, the decoded the Base64 string of the error message reads: Access to Outlook on the web has been blocked by your organization. \u003cBR\u003e\u003cBR\u003eClick \u003ca href=\"/owa/logoff.owa\"\u003ehere\u003c/a\u003e to sign out. What made this a bit more peculiar is that this error was shown only when accessing https://outlook.office.com/ (Outlook on the web) all other M365 services where accessible and working as expected. Initially everyone thought that the error could be caused by a conditional access policy in Microsoft Entra ID, however conditional access policies should trigger at a much earlier stage, during the log-in. After checking the policies we found that there where none configured which would block access to access Outlook on the web (OWA) in Microsoft 365. Next all firewalls and proxies where checked and where possible disabled during troubleshooting. At this point a test set-up was created which allowed us to disable all network routing and security measures. The issue persisted‚Ä¶ Even WireShark traces didn‚Äôt yield any useful information. The only thing that was clear from the traces was that the server was sending a 400 Bad Request response. The Root Cause\rAfter banging our heads on the wall I decided to go back to basics and have a look at the request and response using the developer tools available in any modern browser. This is where I noticed the following payload message: ClientAccessRuleOWAErrorMEssage\rWhile I am far from an expert in Exchange Online I do remember Client Access rules from back in the day when I had a couple of integration challenges with Exchange on-premises. I always assumed that the conditional access policies took over the functionality once provided by Client Access rules. But based on this message it seems that Exchange Online still has this functionality. For those who are not familiar what client access rules are an quick summary. Client Access Rules\rClient Access Rules help you control access to your Exchange Online based on client properties or client access requests. You can prevent clients from connecting to Exchange Online based on their IP address, authentication type, and user property values, and the protocol, application, service, or resource that they‚Äôre using to connect. - source\rA quick search on the interwebs revealed that indeed the Client Access Rules are still available in Exchange Online. However they are deprecated and scheduled for retirement in September 2024. As illustrated by hte following image. Image source The Solution\rSince the Client Access Rules are ancient technology the UI to manage them is not available in the Exchange Online admin center. Instead you have to use the ExchangeOnlineManagement PowerShell module. The following snippet installs this module in the current user scope, imports it, connects to Exchange Online and lists all present Client Access Rules. Just make sure to change the $exchangeAdminUpn value to your Exchange admin. After running the snippet as expected a single Deny Access Client Access Rule was listed with the ‚Äòold‚Äô IP address in the exception list. Active Client Access Rule\rAs a quick fix I extended the exception list using the following snippet with the new IP address. The troublesome err","date":"2024-04-29","objectID":"/posts/2024/400-bad-request-access-to-outlook-on-the-web-has-been-blocked-by-your-organization/:0:0","tags":["Exchange Online","Outlook","Microsoft 365","Troubleshooting"],"title":"üö´ 400 Bad Request. Access to Outlook on the web has been blocked by your organization üõë‚úâÔ∏èüîí","uri":"/posts/2024/400-bad-request-access-to-outlook-on-the-web-has-been-blocked-by-your-organization/"},{"categories":["Azure DevOps","Container Jobs","Containers"],"content":"How to create your own container to use in container jobs?","date":"2024-02-23","objectID":"/posts/2024/how-to-build-and-use-your-own-container-with-azure-devops-container-jobs/","tags":["Container Jobs","Containers","Microsoft Hosted Agents","Self-Hosted Agents","Azure DevOps","Azure Container Registry","Docker","Bicep","Packer","Terraform","Chocolatey","PowerShell"],"title":"üõ†Ô∏è How to Build and Use Your Own Container with Azure DevOps Container Jobs üöÄüì¶","uri":"/posts/2024/how-to-build-and-use-your-own-container-with-azure-devops-container-jobs/"},{"categories":["Azure DevOps","Container Jobs","Containers"],"content":"In my previous post üöÄ Azure DevOps Container Jobs: When Microsoft and Self-Hosted Agents meet, It‚Äôs the Best of Both Worlds! üåêüõ†Ô∏è you where able experience Azure DevOps container jobs. In this post I am delivering on my promise and explain set-by-step how to build and use your own container with Azure DevOps Container Jobs. Buckle up, and let‚Äôs get started! Target set-up\rLets start by having a look at the target set-up. Target set-up\rThe goal of this set-up is to fist build your own container then store the container in a container registry and finally use the container in an Azure DevOps pipeline as a container job. Naturally some infrastructure needs to be in place to make this happen, but thats nothing to worry about as everything needed for to achieve the target set-up is included in this post. With a clear understanding of what we want to achieve lets start building it. Prerequisites\rThe next chapters assume that you have met the following prerequisites: An Azure DevOps Organization with a Microsoft hosted agent is available and accessible to you A project in the concerning Azure DevOps organization exists Contributor and Endpoint Administrator roles granted to your identity in this project A n initialized Git repo in the concerning project is present You have access to an Azure Subscription with contributor permissions Your engineering/development environment has been set up as follow Your environment has the latest PowerShell version installed Your environment has the latest Az module installed Your environment has the latest Bicep version installed Setting up the required infrastructure\rNote\rThe steps in this chapter are to ensure that everyone can follow the instructions disregarding the level of experience with the subjects at hand. If you are already familiar with Infrastructure as Code in Azure and how to set-up Service Connections in Azure DevOps you can probably skip most of the steps if not the whole chapter.\rOur first task is to create a place to store the container after it has been built. For this we are going to use an Azure Container registry. Open a PowerShell terminal and connect to your Azure subscription by running the following snippet. Make sure fill your subscription id and tenant id. Next lets create a resource group by running the following snippet. Feel free to change the resource group name to something of your choice. Now save the following snippet as a file named basicContainerRegistry.bicep to your local machine. The prerequisites to create an Azure Container Registry are met. Run the following snippet in the PowerShell terminal opened in step 1 to start the deployment. Assuming the deployment was successful you should see something like this in the terminal. It‚Äôs also possible to verify the result using the Azure portal. Successful Bicep deployment\rWe are ready to create a service connection in Azure DevOps to the container registry. This service connection will be used to push the container to the just created container registry. Prerequisites\rMake sure you have the necessary permissions to create a service connection in Azure DevOps. Meaning your identity has at least contributor role for the concerning project. Also make sure the same identity has direct or inherited Owner permissions on the newly created Azure Container Registry.\rOpen the Azure DevOps portal and navigate to the project you want to use the container registry in. In the project navigate to Project settings and then Service connections. Click New service connection in the top right corner and select Docker Registry as the connection type, click next. For Registry type select Azure Container Registry and for Authentication Type select Service Principal, a list of subscriptions should be loaded within a couple of seconds. Select the subscription where the newly created container registry is located and select the correct container registry from the list of available container registries. Give the service connection a name, in t","date":"2024-02-23","objectID":"/posts/2024/how-to-build-and-use-your-own-container-with-azure-devops-container-jobs/:0:0","tags":["Container Jobs","Containers","Microsoft Hosted Agents","Self-Hosted Agents","Azure DevOps","Azure Container Registry","Docker","Bicep","Packer","Terraform","Chocolatey","PowerShell"],"title":"üõ†Ô∏è How to Build and Use Your Own Container with Azure DevOps Container Jobs üöÄüì¶","uri":"/posts/2024/how-to-build-and-use-your-own-container-with-azure-devops-container-jobs/"},{"categories":["Azure DevOps","Container Jobs","Containers"],"content":"What are Azure DevOps Container Jobs?","date":"2024-02-13","objectID":"/posts/2024/azure-devops-container-jobs-when-microsoft-and-self-hosted-agents-meet-its-the-best-of-both-worlds/","tags":["Container Jobs","Best of Both Worlds","Microsoft Hosted Agents","Self-Hosted Agents","Azure DevOps"],"title":"üöÄ Azure DevOps Container Jobs: When Microsoft and Self-Hosted Agents meet, It's the Best of Both Worlds! üåêüõ†Ô∏è","uri":"/posts/2024/azure-devops-container-jobs-when-microsoft-and-self-hosted-agents-meet-its-the-best-of-both-worlds/"},{"categories":["Azure DevOps","Container Jobs","Containers"],"content":"Did you know that you can run your Azure DevOps pipeline jobs in a container? It‚Äôs a great way to combine the benefits of Microsoft Hosted Agents and Self-Hosted Agents. In this post, I will explain what Azure DevOps Container Jobs are and how you can use them to your advantage. But first, a recap of Microsoft Hosted Agents and Self-Hosted Agents. What are Microsoft Hosted Agents\rMicrosoft Hosted Agents are a type of agent in Azure DevOps that are hosted and managed by Microsoft. These agents are pre-configured with certain software depending upon the type of agent (Windows, macOS, or Linux) which can run jobs targeting different platforms. With Microsoft Hosted Agents you trade off control for convenience as they are an excellent choice if you need to quickly scale your operations without worrying about the underlying infrastructure management. Pros: Maintenance-Free: Microsoft handles all the maintenance, updates, and upgrades, freeing up your time for other tasks. Scalability: Microsoft Hosted Agents offer excellent scalability (check pricing for cost details), agents are immediately available in your Organization. Simply add or remove them to your liking. Pre-Installed Tools: These agents come with a large variety of pre-installed tools, saving you from manual installation and configuration. Cons: Limited Customization: These agents offer limited scope for customization, which might not be ideal for complex, tailored workflows. Release cadence: Microsoft decides when and what tools to update on the agents, which might introduce breaking changes. Usage Limits: There are usage limitations, which might require you to manage your build and deployment tasks carefully. Less Control: Since Microsoft manages everything, you have less control over the environment and its settings. What are Self-Hosted Agents\rSelf-Hosted Agents are agents in Azure DevOps that you manage and maintain on your own infrastructure. These agents can be installed on local machines, on-premises servers, or even on virtual machines in the cloud. With Self-Hosted Agents you trade off convenience for control as they are a great choice if you need more control over the environment and its settings, or if you need to use specific versions of tools and software. Pros: Greater Control: With Self-Hosted Agents, you have complete control over the environment, settings, and installed tools, providing flexibility for complex workflows. No Usage Limits: There are no strict usage limitations, allowing for more extensive and continuous usage. Customizable Setup: You can tailor the setup to match your specific requirements, including installing specific versions of tools and software. Cons: Requires Maintenance: Unlike Microsoft Hosted Agents, you are responsible for maintaining, upgrading, and updating the Self-Hosted Agents. Infrastructure Costs: You‚Äôll need to account for the costs of the infrastructure needed to host these agents. Delayed Access to New Features: New features and improvements will not be immediately available, as they depend on when you update your agents. Azure DevOps Container Jobs: The Best of Both Worlds\rAzure DevOps Container Jobs operate by running each job in an isolated, consistent container which you specify in your pipeline YAML code. You can reference a container from a public or a private repository (just make sure your service connection has sufficient privileges to the repository at hand). This means you can have the convenience and scalability of Microsoft Hosted Agents, while also having the control and flexibility of Self-Hosted Agents. Let‚Äôs take a look at the main benefits. Customization: Like Self-Hosted Agents, container jobs can be highly customized. You can choose the base images, install specific tools and dependencies, and set environment variables. This makes it ideal for complex workflows that require specific configurations, providing the flexibility similar to Self-Hosted Agents. Maintenance-free: Container jobs, like Micro","date":"2024-02-13","objectID":"/posts/2024/azure-devops-container-jobs-when-microsoft-and-self-hosted-agents-meet-its-the-best-of-both-worlds/:0:0","tags":["Container Jobs","Best of Both Worlds","Microsoft Hosted Agents","Self-Hosted Agents","Azure DevOps"],"title":"üöÄ Azure DevOps Container Jobs: When Microsoft and Self-Hosted Agents meet, It's the Best of Both Worlds! üåêüõ†Ô∏è","uri":"/posts/2024/azure-devops-container-jobs-when-microsoft-and-self-hosted-agents-meet-its-the-best-of-both-worlds/"},{"categories":["PowerShell"],"content":"The most essential PowerShell Commands","date":"2023-10-29","objectID":"/posts/2023/powershell-prowess-the-3-commands-you-need-to-know/","tags":["Need to know","PowerShell","Get-Member","Select-Object","Type Accelerators","DataType.FullName"],"title":"PowerShell Prowess: The 3 commands you need to know üëà","uri":"/posts/2023/powershell-prowess-the-3-commands-you-need-to-know/"},{"categories":["PowerShell"],"content":"Next to the Get-Command and hte Get-Help cmdlets, there are 3 other cmdlets that are essential for any PowerShell user. In this blog post I will explain my view on why they are essential and provide hands-on examples on how to use them. Unveiling Object Properties and Methods with Get-Member\rThe official documentation states: The Get-Member cmdlet gets the members, the properties and methods, of objects. Making it the Swiss army knife for exploring the properties and methods of any object in PowerShell and thus indispensable for any PowerShell user. Why you need to know Get-Member: Object Exploration -\u003e When working with PowerShell objects it‚Äôs crucial to understand the object you are dealing with. Get-Member helps to uncover the object‚Äôs properties, methods, and other characteristics. Discover Object Type -\u003e Get-Member can be used to determine the type of an object. This is especially useful when working with pipelines, as different cmdlets return objects of varying types. Get-Member Example\rIn this example the Get-MgUser cmdlet is used to retrieve a user from the Microsoft Graph. Note the result: Get-MgUser cmdlet returns the bare minimal properties\rNow note the Get-Member output: By pipeing the command to Get-Member all properties, methods, and other characteristics are now visible and explorable\rNote\rEasily overlooked, but as mentioned the Get-Member cmdlet also returns the objects TypeName.\rTailoring Object Output with Select-Object\rThe official documentation states: Selects objects or object properties. The cherry picker for displaying and formatting specific properties of an objects. Making it an invaluable for formatting and refining data. This is a versatile command that enables customization of the properties of an object‚Äôs output. Why you need to know Select-Object: Data Shaping -\u003e Tailoring object output to specific needs by including, excluding ordering and filtering properties. This is crucial for creating neat and organized objects. Efficient Scripting -\u003e When working with complex objects, displaying all available properties with Select-Object -Property * helps by making output transparent and easier to work with. Select-Object Example\rThe example is a continuation of the previous one. First all properties are listed. Then a selection is made of the preferred properties. Finally the selected properties for the last 3 objects are returned. Listing all properties using the -Property * command\rSelecting specific properties\rReturning the selected properties from the last 3 objects\rThe Full Name(space) of a Data Type with [DataType].FullName\rOutside of folder and file paths this method is a less known one and requires a bit more explanation. PowerShell uses a concept called Type Accelerators to make it easier to work with .NET types. A Type Accelerator is a shortcut for a .NET type. For example [int] is a shortcut for the .NET type System.Int32. While the intent of the type accelerators is to shorten the names of .NET types there are reasons to use the full name of a type. Such is the case when using the New-Object cmdlet to create an instance of a .NET type which can be retrieved by using the [DataType].FullName method. Another consideration for using the full name comes from the best practice to avoid aliases (PSAvoidUsingCmdletAliases), since the type accelerators are aliases for .NET types some PowerShell devs (like myself) prefer using the .NET types. [DataType].FullName Example\rTo get a list of all available type accelerators use the following command: List of all Type Accelerators\rExample of the most common Type Accelerators full names\rWrapping up\rThat‚Äôs all folks! I hope you found the Get-Member, Select-Object and the DataType Accelerators .FullName commands as useful and need to know as I do. If you are interested in the reference material used to make this post, please visit the following links. Get-Member learn.microsoft.com - Get-Member YouTube video by David Dalton - Exploring PowerShell objects with Get","date":"2023-10-29","objectID":"/posts/2023/powershell-prowess-the-3-commands-you-need-to-know/:0:0","tags":["Need to know","PowerShell","Get-Member","Select-Object","Type Accelerators","DataType.FullName"],"title":"PowerShell Prowess: The 3 commands you need to know üëà","uri":"/posts/2023/powershell-prowess-the-3-commands-you-need-to-know/"},{"categories":["Starter kit","Infrastructure as Code","Git","Azure"],"content":"Azure IaC repository structure starter kit","date":"2023-10-15","objectID":"/posts/2023/chaos-to-order-structuring-iac-repository-like-a-boss/","tags":["Template","Starter Kit","Git repository","IaC","Bicep","Terraform","ARM","Azure","Infrastructure as Code","PowerShell"],"title":"Chaos to Order: Structuring IaC repository Like a Boss! üëë","uri":"/posts/2023/chaos-to-order-structuring-iac-repository-like-a-boss/"},{"categories":["Starter kit","Infrastructure as Code","Git","Azure"],"content":"With this post I want to share my new GitHub repository -\u003e Starter Kit for an Azure IaC repository. This starter kit is based on best practices, personal experiences, and guidelines for creating and organizing code and resources tailored for Azure infrastructure deployment. While the example itself is based on a combination of Azure DevOps, Bicep and PowerShell, the concepts are applicable to any IaC language and tool combination. Why a starter kit?\rThe goal of this repository is to help engineers reduce their start-up time by providing the necessary structure. The goal of this post is to elaborate on the repository structure and provide some background information. The starter kit\rThe starter kit is composed of 5 top level folders with üìÇ.vscode folder which contains my configuration for Visual Studio Code. The üìúextensions.json file contains the extensions needed by most Microsoft Azure IaC projects. The most notable config in the üìúsettings.json is the tab size, PowerShell code formatting and the horizontal editor rulers set to 80, 100 and 120 characters. These ruler settings represent the Guidance for contributing to Microsoft Docs and the PSScriptAnalyzer rule AvoidLongLines. Like all other parts of the starter kit, feel free to change settings to your liking. In addition to the top level folders the starter kit also has a generic üìú.gitignore and the main üìú.README.md which contains the description for this kit. Obviously the üìúLICENSE file is not part of the starter kit. Each folder is described in a separate chapter of this blog post, enjoy the read! The main tree view of the Starter Kit for an Azure IaC repository is shown below. üì¶azure-iac-repo-structure-starter-kit ‚î£ üìÇ.vscode ‚î£ üìÇdocs ‚î£ üìÇpipeline-gallery ‚î£ üìÇpipelines ‚î£ üìÇsrc ‚î£ üìÇtools ‚î£ üìú.gitignore ‚î£ üìúLICENSE ‚îó üìúREADME.mdDocs\rMoving trough the top level folders, the first one is the üìÇdocs folder. This is the folder to keep all the technical documentation considered relevant for the IaC files and scripts. Following hte everything as code mantra the documentation must be written in Markdown. Each markdown file should cover a single topic and should be named accordingly. Since each project tends to have it‚Äôs own documentation requirements, the documentation folder is not further structured into subfolders. Pipeline gallery\rThe pipeline-gallery folder is the place to keep all the pipeline templates. Usage of pipeline templates encourages reusability by bootstrapping new pipelines. In addition the use of pipeline templates positively impacts the maintainability and testing of pipelines as each template cna be tested in isolation. As the concept of this folder is based on the DRY principle the added value of this folder depends on the reusability scale of the pipelines templates. If your needs require the reuse the templates across multiple repositories opting for a specific repository with an Artifacts feed might be a more suited option. For consistency reasons the folder is structured into two subfolders which are based on the pipeline type. With the üìÇjobs folder for all the job templates and the üìÇsteps folder for all the step templates. Populated with files, it looks something like this: üì¶pipeline-gallery ‚î£ üìÇjobs ‚îÉ ‚î£ üìúdeploySolution.yml ‚îÉ ‚î£ üìúdeployInfrastructure.yml ‚îÉ ‚î£ üìústaticAnalysis.yml ‚îÉ ‚îó üìúcodeQualityValidation.yml ‚î£ üìÇsteps ‚îÉ ‚î£ üìúbuildSolution.yml ‚îÉ ‚î£ üìústaticAnalysis.yml ‚îÉ ‚î£ üìúpublishSolutionArtifacts.yml ‚îÉ ‚îó üìúbuildBicep.yml ‚îó üìúreadme.mdPipelines\rThe pipelines folder is the place to keep all the pipeline definitions. Each pipeline definition is placed inside a subfolder which is named after the pipeline. Depending on the design of the pipeline, there are two options to choose from: A single multistage üìú.yml file that contains all the stages or environments. Multitude of üìú.yml files to represent each stage or environment. And finally a üìúreadme.md that contains technical documentation elaborating on the specifics of the respective pipeline is required. Populated with files, it","date":"2023-10-15","objectID":"/posts/2023/chaos-to-order-structuring-iac-repository-like-a-boss/:0:0","tags":["Template","Starter Kit","Git repository","IaC","Bicep","Terraform","ARM","Azure","Infrastructure as Code","PowerShell"],"title":"Chaos to Order: Structuring IaC repository Like a Boss! üëë","uri":"/posts/2023/chaos-to-order-structuring-iac-repository-like-a-boss/"},{"categories":["Cloud Governance","Naming Conventions","Azure"],"content":"A quick and Easy Way to Add Renamable Display Names to Azure Resources","date":"2023-08-29","objectID":"/posts/2023/a-quick-and-easy-way-to-add-renamable-display-names-to-azure-resources/","tags":["Naming convention","Azure","Resources","Cloud","Resource naming","Resource Display Name"],"title":"A quick and Easy Way to Add Renamable Display Names to Azure Resources üí®","uri":"/posts/2023/a-quick-and-easy-way-to-add-renamable-display-names-to-azure-resources/"},{"categories":["Cloud Governance","Naming Conventions","Azure"],"content":"In this post I will share a couple of tips with regards to Secrets Management in your (local) dev environment. Personally I use these to prevent / limit leaking of secrets while developing Azure infrastructure. And I consider them as part of my security hygiene during the development process. Last year I wrote a series of posts covering the naming convention topic. The perfect azure naming convention covered the Azure resources. The primary concept for this naming convention is that in fact Azure resource names are actually ID‚Äôs which cannot be renamed. In this post I want to share a method with which renamable display names can be added. How to add renamable display names to Azure resources\rThere should be little surprise that the renamable display name is a resource tag. In this specific case it‚Äôs a special (hidden) tag called hidden-title. When this tag is added it‚Äôs value is used as the display name for the resource in question in the Azure portal. Even if the tag is hidden it can be still set using the Azure portal, but it disappears in the portal as soon as you leave the tags page. To change the value via the portal, add it again with the new value then wait a minute and do a Forced Reload (Ctrl F5). Naturally adding this tag is also possible using infrastructure as code (Bicep, Terraform, ARM, etc‚Ä¶) or scripting (PowerShell, Azure CLI, etc‚Ä¶). Check out the following images to see both approaches and their result. Renamable display names via the portal\rRenamable display names via scripting\rRenamable display names result\rFor convenience a snippet of the above used code. Did I also mention that this also works on resource groups? Yes, it also works on resource groups\rInfo\rKeep in mind that since the display name is a tag the limitations that apply to the tags are applicable.\rBy now you are probably wondering if is this a supported feature? Yes its a supported feature, but the main purpose of this feature is to provide resource display names for Azure dashboards views. And to make sense of data in dashboards the same display names are also made visible in the Azure portal. The documentation for this feature is unfortunately similarly to the tag itself, hidden‚Ä¶. So I dug the links up for your convenience, check them out here and here. Practical applications\rI am still firmly convinced a proper naming convention is a must for any Cloud environment and Azure resource names should be treated as ID‚Äôs due to their immutable nature. However this is a good addition which can help smoothen the transition for any organization from a traditional pets to a cattle approach without sacrificing pet names üòâ. No real dogs where harmed as this image is AI generated\rLimitations\rInfo\rUPDATE 2023-09-07 Due to lots of interest for this topic some questions arose with regards to possible limitations, therefore I decided to add this chapter.\rI have experimented with different views and identified a couple of limitations. The first one is in the main search (located at the top of the screen) and the second one is in the Tags and Tags filter. Main Azure Portal search\rWhen using a part or the full value of the hidden-title tag in the main search window, the tagged resources are shown in the search results but the display names added via the hidden-title tag are only visible in the mouseover. Check out the next image, it contains two examples of this limitation. Search works but not as expected\rTags en Tags filters\rFor the Tags and tags filtering this functionality does not support tags prefixed with hidden-. Check out the next image, it contains the examples of these limitations. Limitations are mostly present in the Tags views and commands\rWrapping up\rI hope you found this neat trick to add renamable display names to Azure resources useful. If you are interested in the reference material used to make this post, please visit the following links. Microsoft docs - hidden-title tag Pets vs cattle explained Tags As always, a big thanks for reading this p","date":"2023-08-29","objectID":"/posts/2023/a-quick-and-easy-way-to-add-renamable-display-names-to-azure-resources/:0:0","tags":["Naming convention","Azure","Resources","Cloud","Resource naming","Resource Display Name"],"title":"A quick and Easy Way to Add Renamable Display Names to Azure Resources üí®","uri":"/posts/2023/a-quick-and-easy-way-to-add-renamable-display-names-to-azure-resources/"},{"categories":["Cloud Adoption Framework","Azure Migration and Modernization"],"content":"An Azure DevOps backlog confining the ready phase of the Microsoft Cloud Adoption Framework","date":"2023-07-31","objectID":"/posts/2023/mastering-cloud-adoption-framework-quick-start-backlog-essentials/","tags":["Microsoft","Cloud Adoption Framework","Azure DevOps","Azure DevOps Demo Generator","Azure Migration and Modernization","Template","Backlog"],"title":"Mastering Cloud Adoption Framework: Quick Start Backlog essentials üí™","uri":"/posts/2023/mastering-cloud-adoption-framework-quick-start-backlog-essentials/"},{"categories":["Cloud Adoption Framework","Azure Migration and Modernization"],"content":"The Microsoft Cloud Adoption Framework is a comprehensive set of guidelines, best practices, tools, and documentation. But, this comprehensiveness can render the framework complex and potentially overwhelming for individuals and organizations, most notably for those with limited cloud expertise. As an attempt to offset the complexity I created a ready to use quick start backlog for the Design Areas sub-section of the Microsoft Cloud Adoption Framework. Via this blog post I want to share it, explain the composition and guide you on how import it into your own Azure DevOps organization within minutes. Live demo and download of the backlog\rIn case you want to go right ahead with the backlog, please use the following links. If you want to know more about the backlog, please continue reading the rest of the post :) Live demo - The DevJevNL Cloud Adoption Plan backlog Download - The DevJevNL Cloud Adoption Plan backlog Backlog composition\rThe backlog uses the scrum template and hold a total of 268 backlog items! These 268 items are divided into different work item types as shown below: Epics: 9 Features: 59 Product Backlog Items: 172 Tasks: 28 Each epic represents a Design Areas topic with the exception of the Security topic. The sub-topic Securing privileged access for hybrid and cloud deployments in Azure AD is covered by a dedicated epic to emphasize it‚Äôs size and complexity. Backlog and Cloud Adoption framework topics\rNext each epic is devided into features where each feature represent a sub-topic of the epic in question. Backlog and Cloud Adoption framework sub-topics\rThe features are then further divided into product backlog items which represent the actionable items that need to be completed to achieve the feature in question. In some cases the product backlog items are further divided into tasks to make the work more manageable. Combine result looks like shown below. Backlog and Cloud Adoption framework product backlog items\rcompanion to it. How to import the backlog into your Azure DevOps organization\rTo import the backlog into your Azure DevOps organization, you need to use the Azure DevOps Demo Generator. The Azure DevOps Demo Generator is a community project that helps with the creation of Azure DevOps projects with pre-populated (sample) content that includes source code, work items, iterations, etc‚Ä¶ It leverages the Azure DevOps REST API to provision data. For more details of this tool please check my previous blog post Use Azure DevOps Demo Generator to create a new Project in Azure DevOps. To import the backlog into your Azure DevOps organization, follow the steps below. Navigate to the Azure DevOps Demo Generator website. Click on the Sign in button in the top right corner of the page. Azure DevOps Demo Generator - Sign in\rSign in with your Azure DevOps credentials (make sure the account in question has the correct permissions) to create Azure DevOps projects. Click on the Choose template button. Azure DevOps Demo Generator - Create a new project\rIn the choose template page, select the private option. Select the radio button GitHub and enter the following URL: https://github.com/thecloudexplorers/best-practices-framework/blob/master/the-devjevnl-cloud-adoption-plan-backlog/the-devjevnl-cloud-adoption-plan-backlog.zip. Finish by clicking submit. Azure DevOps Demo Generator - Select the template\rNow click submit and wait for the import to complete. Azure DevOps Demo Generator - Importing\rOnce the import is completed, click on the Navigate to project button to review the imported backlog. Azure DevOps Demo Generator - Finished importing\rTo make the whole structure visible select the Epics option from the backlog item type filter (if the option is missing click on hte cogwheel next to it and ensure hte checkbox in from of Epics is visible). Azure DevOps Demo Generator - Set the correct view level\rEnjoy your backlog! It should looks something like this. Azure DevOps Demo Generator - Success!\rWrapping up\rI hope you find t","date":"2023-07-31","objectID":"/posts/2023/mastering-cloud-adoption-framework-quick-start-backlog-essentials/:0:0","tags":["Microsoft","Cloud Adoption Framework","Azure DevOps","Azure DevOps Demo Generator","Azure Migration and Modernization","Template","Backlog"],"title":"Mastering Cloud Adoption Framework: Quick Start Backlog essentials üí™","uri":"/posts/2023/mastering-cloud-adoption-framework-quick-start-backlog-essentials/"},{"categories":["Secure Development","PowerShell"],"content":"Don't spill the beans - keep your secrets secure","date":"2023-06-26","objectID":"/posts/2023/dont-spill-the-beans-keep-your-secrets-secure/","tags":["Cloud Security","Cloud Automation","PowerShell","Way of Working","Local Dev","Security Hygiene","SecretManagement","SecretStore"],"title":"Don't spill the beans - keep your secrets secure ü§´","uri":"/posts/2023/dont-spill-the-beans-keep-your-secrets-secure/"},{"categories":["Secure Development","PowerShell"],"content":"In the previous post Sleep Worry-Free: The best tips for Local Secrets Management I shared the best tips for managing local secrets. In this post I will take Secrets Management to the next level by showing you how to create manage and use secrets securely by setting up a local secure store using PowerShell SecretManagement and PowerShell SecretStore modules. Modules explained\rSince this approach to Secrets Management depends on the two just mentioned modules the purpose of each of the modules is explained next. PowerShell SecretManagement module\rImportant to note here is that this module does not store any secrets it is a uniform interaction layer to to access and manage secrets stored different kind of extension vaults. This module supports managing the following secret data types: byte[] string SecureString PSCredential Hashtable PowerShell SecretStore\rThe SecretStore module is an extension vault for the just mentioned PowerShell SecretManagement module. This is the module that stores secrets and it does so locally within the current user context. It uses the .NET crypto APIs to encrypt the file which is used ot store the secrets. Since this modules works in all by PowerShell supported Operating systems it runs on Windows, Linux and macOS. By default the store requires a password for unlocking it, this is done to provide the strongest. As all other extension vaults this vault is registered to the current logged in user context, and is available only to the user in question. The extension vault registry file is stored in the user account protected directory. For Windows platforms the location is: %LOCALAPPDATA%\\Microsoft\\PowerShell\\secretmanagement For non-Windows platforms the location: $HOME/.secretmanagement Set-up your Secure Store\rThe first step is to make sure that the two modules are installed and loaded, namely; Microsoft.PowerShell.SecretStore Microsoft.PowerShell.SecretManagement For this I used a simple ForEach loop which checks if the modules are already installed both are loaded if not both are installed and then loaded. The snippet I use is as follows: Register a new vault\rTo register a new vault a SecureString password is required. This password is exported into an XML file and encrypted by Windows Data Protection (DPAPI) using the Export-Clixml command. Next a new vault is created using the Register-SecretVault command. To complete the registration of the vault the just created password is used to secure it and the following configurations values are also set. Authentication \u003c- Setting this parameter to Password Ensures a password is required to secure the concerning vault PasswordTimeout\u003c- The amount of time the vault stays unlocked after using the password, the default is 15 minutes however I like to set it to 3600 seconds (1 hour) Interaction \u003c- Specifies whether the SecretStore should prompt a user when they access it. I used none to be able to run the whole thing via an automation script. Password \u003c- the actual password encrypted using Export-Clixml. Needed when previous parameter is set to none. Confirm \u003c- Prompts you for confirmation before running the cmdlet. I set it to $false since I intend to execute it via an automation script. I combine the above settings into a function based on the following snippet: Use the new vault\rThe first step to start using the newly created vault is to make sure it‚Äôs unlocked. For this the password used during the creation must be provided using the Unlock-SecretStore command. With the vault unlocked the Set-Secret and Get-Secret can be used to store and retrieve secrets. Info\rAn interesting mention is that it is possible set an expiration date using the -Metadata parameter. Naturally the extension vault itself must support it this as not all do.\rBefore starting a debug session I simply use the Set-Secret command to store te required secrets in the vault and use them in my throw away / one time use PowerShell script Jevs-Playground.ps1. For details on this script please ","date":"2023-06-26","objectID":"/posts/2023/dont-spill-the-beans-keep-your-secrets-secure/:0:0","tags":["Cloud Security","Cloud Automation","PowerShell","Way of Working","Local Dev","Security Hygiene","SecretManagement","SecretStore"],"title":"Don't spill the beans - keep your secrets secure ü§´","uri":"/posts/2023/dont-spill-the-beans-keep-your-secrets-secure/"},{"categories":["Secure Development","PowerShell"],"content":"Sleep Worry-Free: The best tips for Local Secrets Management","date":"2023-06-19","objectID":"/posts/2023/sleep-worry-free-the-best-tips-for-local-secrets-management/","tags":["Cloud Security","Cloud Automation","PowerShell","Json","Way of Working","Local Dev","Security Hygiene"],"title":"Sleep Worry-Free: The best tips for Local Secrets Management üí§","uri":"/posts/2023/sleep-worry-free-the-best-tips-for-local-secrets-management/"},{"categories":["Secure Development","PowerShell"],"content":"In this post I will share a couple of tips with regards to Secrets Management in your (local) dev environment. Personally I use these to prevent / limit leaking of secrets while developing Azure infrastructure. And I consider them as part of my security hygiene during the development process. Use Case\rWhen you are either building new Azure Infrastructure or debugging an existing configuration you will eventually need to use a secret or a password on you local (dev) environment. The most straightforward approach that is often used is to create a new variable and add the secret as a value to this variable. Effectively (temporarily) hardcoding the secret value into whatever you are working on. The following example sows such an approach, it should be quite familiar to everyone who worked with Azure and PowerShell in the past. In this example the secret is part of a temporary or a one time use script created to establish a connection with Azure. Which is needed to for example carry out a deployment. What happens after such a script is used is where the risks lie. More often then not the secrets are left part of the script with the intent to remove them later on, however due to various reasons like time pressure this never happens and in some cases another debug snippet is added to the same or a new file with in some cases new set of secrets. Secrets used in such way have become un-managed and pose a potential security security risk. Take the following scenario‚Äôs as examples of how this can cause the secrets to be leaked. The script with the secret can ben accidentally checked in making it available to everyone who has access to the repository in question. The engineer forgets about the secret leaving it hardcoded in the file which on a much later moment is shared with others as its just a simple draft. The engineer shares his screen allowing for a screenshot to be taken or the whole thing recorded. Thankfully there are a multiple actions that can be taken to reduce and even prevent this. The next chapter describes my method for Secrets Management. How to manage your secrets - private config\rThis method is what I consider a structured approach to organizing a repository. In essence it is an implementation of the separation of credentials from code practice. To achieve this my repositories always have the folder set-up as shown in the next example. Note\rThis example show only a relevant subset of the whole repository folder structure.\rThe tools and references folders contain any tools and any reference material used by the code in the rest of the repo. The personalScripts folder is the folder that is used by the team and me for any temporary and one time use script. Such as snippets to connect to Azure, Azure DevOps or as a simplified controller to call external functions during debugging. I always start by adding this folder .gitignore to ensure no accidental check-in‚Äôs which result in secret leaking. The readme.md in this folder is excluded from .gitignore and is part of source controlled files. It contains a description of how to use this set-up. This is the .gitignore snippet for such a set-up: jevs.private.config.json\rThis is the file I use to store all relevant credentials, secrets and other type of GUID and id‚Äôs which are relevant for the task at hand. I use json as it is really easy to work with when combining it with PowerShell. Putting all the sensitive information which is required for my task in a single file and combining it with .gitignore guardrails allows me have Managed Secrets. Note\rAll files containing .private.config. are omitted from source control explicitly.\rJevs-Playground.ps1\rThis is my throw away / one time use PowerShell script which a I modify to fit my task at hand. It contains a header region which is relatively static as this region loads the values from the just mentioned jevs.private.config.json.. The script can be seen next. Wrapping up\rAnd there you have it, the best tips I can share from my own ex","date":"2023-06-19","objectID":"/posts/2023/sleep-worry-free-the-best-tips-for-local-secrets-management/:0:0","tags":["Cloud Security","Cloud Automation","PowerShell","Json","Way of Working","Local Dev","Security Hygiene"],"title":"Sleep Worry-Free: The best tips for Local Secrets Management üí§","uri":"/posts/2023/sleep-worry-free-the-best-tips-for-local-secrets-management/"},{"categories":["Azure DevOps Fundamentals"],"content":"How to use Library for Azure Pipelines","date":"2023-03-21","objectID":"/posts/2023/how-to-use-library-for-azure-pipelines/","tags":["Azure DevOps","Fundamentals","Library for Azure Pipelines"],"title":"How to use Library for Azure Pipelines üìó","uri":"/posts/2023/how-to-use-library-for-azure-pipelines/"},{"categories":["Azure DevOps Fundamentals"],"content":"This is the 6th post in the category Azure DevOps Fundamentals of the blog post series on working with Azure DevOps In this post the I will show how to use Library for Azure Pipelines. Since it much more fun to do research and play with technology in collaboration with others, Wesley Camargo will be covering some of the topics on he‚Äôs blog, while other topics are covered by my here. Naturally the all related posts will be referenced between the two blogs. What is a Library for Azure Pipelines\rA Library for Azure Pipelines is an Azure DevOps feature that enables asset management for an Azure DevOps Project. Asset can be either defined in the form of variables which are grouped together in Variable Groups or as so called Secure Files. All defined assets can be used in multiple build and release pipelines of an Azure DevOps Project. Although not really recommended it is also possible to modify permissions to a degree that these asses are accessible by pipelines present in other Azure DevOps Projects within the same Azure DevOps organization. Note\rIn Azure DevOps library features like Secure files and Secret variables are secured in the same way as Service connections as all are considered protected resources Secrets are encrypted and stored in the database. The keys to decrypt secrets are stored in Azure Key Vault. The keys are specific to each scale unit. So, two regions don‚Äôt share the same keys. The keys are also rotated with every deployment of Azure DevOps. The rights to retrieve secure keys are only given to the Azure DevOps service principals and (on special occasions) on-demand to diagnose problems. The secure storage doesn‚Äôt have any certifications. How to create a Variable Group\rVariable groups are used to store values in the form of key value pairs where a variable name is used as a key. In addition it is possible to link an Azure Key Vault as variables of a Variable group. Variables groups can be directly referenced in one or more YAML pipelines in the same project. When defining variables in a variable group it is possible to mark multiple variables as a secrets. When done so these variables are then considered by Azure DevOps as protected resources. This allows for various combinations of approvals, checks, and pipeline permissions to be added to limit access to the concerned secrets in a variable group. Info\rAccess to non-secret variables is not limited by approvals, checks, and pipeline permissions.\rLets go ahead an create a couple of variables. Assuming you have your Azure DevOps project open, click in the left hand menu on Pipelines then click on Library. Library\rClick on either the +Variable group button in the middle of the screen or the one located in the horizontal menu. New Variable group\rAdd a Variable group name, and a Description. There are no specific character limitations for the Variable group name, it is not desired but possible to start the name with a # or a @. A Variable Group cannot be saved if there are no variables in it. Proceed by adding a new variable by clicking on the +Add button, use a name and value you deem fit. Click the Save button in the horizontal menu. The result should be similar to the following image. New Variable\rNext add an additional variable by clicking on the +Add button. Again use a name and value ad you deem fit. Before saving click on the lock icon on the right side of the new variable to change the type to secret. As show in the following image click the Save button. New Secret\rIn the above step I just created a regular variable and one secret. The regular one is straight forward however the secrets are treated differently. The following applies to secrets: Secrets are considered protected resources Secrets are encrypted at rest with a 2048-bit RSA key Secrets are available on the agent for tasks and scripts to use (be careful about who has access to modify pipelines). Secrets are unlike regular variables not automatically decrypted into environment variables for script","date":"2023-03-21","objectID":"/posts/2023/how-to-use-library-for-azure-pipelines/:0:0","tags":["Azure DevOps","Fundamentals","Library for Azure Pipelines"],"title":"How to use Library for Azure Pipelines üìó","uri":"/posts/2023/how-to-use-library-for-azure-pipelines/"},{"categories":["Azure DevOps Fundamentals"],"content":"Use Azure DevOps Demo Generator to create a new Project in Azure DevOps","date":"2023-02-15","objectID":"/posts/2023/use-azure-devops-demo-generator-to-create-a-new-project-in-azure-devops/","tags":["Azure DevOps","Fundamentals","New Project","Azure DevOps Demo Generator"],"title":"Use Azure DevOps Demo Generator to create a new Project in Azure DevOps üìë","uri":"/posts/2023/use-azure-devops-demo-generator-to-create-a-new-project-in-azure-devops/"},{"categories":["Azure DevOps Fundamentals"],"content":"This is the 5th post in the category Azure DevOps Fundamentals of the blog post series on working with Azure DevOps In this post the I will introduce you to the Azure DevOps Demo Generator tool by using it to create a new Project in Azure DevOps. Since it much more fun to do research and play with technology in collaboration with others, Wesley Camargo will be covering some of the topics on he‚Äôs blog, while other topics are covered by my here. Naturally the all related posts will be referenced between the two blogs. What is an Azure DevOps Demo Generator\rThis is a tool or as Microsoft documentation references it, a service which is aimed at provisioning pre-populated content that includes source code, work items, iterations, service connections, and build and release pipelines based on based on a subset of out of the box available templates. In addition this service enables the users to make their own templates by converting existing projects into templates and use these to create new fully decked out projects. Use Azure DevOps Demo Generator to create a new Project in Azure DevOps\rPrerequisite\rPrior to continuing with the next steps make sure that a suitable Azure DevOps Project is available and that you have sufficient permission to the project in question. To avoid any issues during the process Project Administrator permissions are recommended.\rLets get our hands dirty! First Create a custom Azure DevOps Demo Generator template\rOpen Azure DevOps Demo Generator using the following url https://azuredevopsdemogenerator.azurewebsites.net/?enableextractor=true Note the query parameter enableextractor=true. This parameter enables the custom template functionality On the main screen click the Sig In button Sign in using - Azure DevOps Demo Generator\rReview the required authorizations carefully, click the Accept button to be able tto proceed Manage authorization - Azure DevOps Demo Generator\rClick the Build your template link located in the top right corner Build your template - Azure DevOps Demo Generator\rSelect the Organization, then select the project that is desired to be used as a template and finally click the Analyze button Analyze - Azure DevOps Demo Generator\rThe project in question is now validated for a supported process and if all part of the project are accessible within with the account in question. Get a cup of coffee as with really large template Projects this could take a couple of minutes. Assuming the analysis is successful you should see a report with the Generate Artifacts button at the bottom, click it to continue. Artifact Analysis - Azure DevOps Demo Generator\rIf everything went well the artifact summary should show all green check boxes. In addition a Click to download your file link should be available to download the Artifact. Make sure to save the artifact in a proper place as after the download it will be deleted from the Azure DevOps Demo Generator Download Artifact - Azure DevOps Demo Generator\rAs a result you should have a .zip file downloaded. Zip file - Azure DevOps Demo Generator\rCreate a new Project using the custom Azure DevOps Demo Generator template\rIf not already open, open the Azure DevOps Demo Generator tool once again using the following url https://azuredevopsdemogenerator.azurewebsites.net/?enableextractor=true Click the Choose template button Choose template - Azure DevOps Demo Generator\rIn the new window click the Private link Private - Azure DevOps Demo Generator\rThe windows is actually quite interesting as it is possible to supply a Github or even an URL reference in addition to the Local Drive option. For the Azure DevOps Fundamentals category the Local Drive option will suffice. Ensure that Local Drive option is selected. Then click the Choose File button and select the in the previous chapter generated .zip file. When done click the Submit button Submit file - Azure DevOps Demo Generator\rNote that you are returned to the main page with the Selected Template field pre-populated wit","date":"2023-02-15","objectID":"/posts/2023/use-azure-devops-demo-generator-to-create-a-new-project-in-azure-devops/:0:0","tags":["Azure DevOps","Fundamentals","New Project","Azure DevOps Demo Generator"],"title":"Use Azure DevOps Demo Generator to create a new Project in Azure DevOps üìë","uri":"/posts/2023/use-azure-devops-demo-generator-to-create-a-new-project-in-azure-devops/"},{"categories":["Azure DevOps Fundamentals"],"content":"How to create a new Azure Service Connection in Azure DevOps","date":"2023-01-20","objectID":"/posts/2023/how-to-create-a-new-azure-service-connection-in-azure-devops/","tags":["Azure DevOps","Fundamentals","New Service Connection"],"title":"How to create a new Azure Service Connection in Azure DevOps üìò","uri":"/posts/2023/how-to-create-a-new-azure-service-connection-in-azure-devops/"},{"categories":["Azure DevOps Fundamentals"],"content":"This is the 4th post in the category Azure DevOps Fundamentals of the blog post series on working with Azure DevOps. In this post the I will demonstrate how to create an Azure Service Connection in an Azure DevOps Project. Since it much more fun to do research and play with technology in collaboration with others, Wesley Camargo will be covering some of the topics on he‚Äôs blog, while other topics are covered by my here. Naturally the all related posts will be referenced between the two blogs. What is an Azure Service Connection\rAn Azure Service Connection or officially called ‚ÄòAzure Resource Manager Service Connection‚Äô allows for Pipelines in Azure DevOps to connect to Azure for the purpose of executing Azure Resource Manager related tasks. For example, to deploy infrastructure using the Infrastructure as Code method. There are 4 authentication methods available to choose from when creating an Azure Service Connection. Each method allows Pipelines in Azure DevOps to connect to Azure using its own authentication approach. Service principal (automatic) Service principal (manual) Managed identity Publishing Profile Since the Service principal (automatic) method is marked as recommended in Azure DevOps I will cover this method as it would fit best in being part of the Azure DevOps Fundamentals category. When used, this method creates a n Azure Service connection that is composed of two parts. One part resides in Azure DevOps and the other one resides in Azure Active Directory. The first part contains all the necessary configuration to be able to use the service connection as part of Pipelines in Azure DevOps. The second part is actually an App registration (see it as a traditional service account). This app registration is created in the background during the Azure Service Connection creation process. The next chapters depict and explain both the first and the second part. How to create a new Azure Service Connection in Azure DevOps\rBefore jumping in, lets get the prerequisites clear. Prerequisites\rIn the Azure DevOps Project you need to be signed in as the owner of the Azure Pipelines Organization In Azure you need to be signed in as the owner of an Azure Subscription (the one you want the service connection connect to) If multiple accounts are used for the previous two points make sure you sign-in with both accounts in the same browser instance In Azure Active Directory the user in question must have Directory Read role assigned In Azure Active Directory the user in question must have at least one of the following roles, which role is needed depends on the type of user and the Users can register applications setting. Application Administrator Application Developer Cloud Application Administrator Make sure you are on the landing page of the Azure DevOps project in question. Click in the top left bottom corner on Project settings link. Project settings\rIn the left hand navigation menu click on Service connections. Service connections\rNow click on the Create service connection button in the middle of the screen. Create service connections\rOn the New service connection page select the Azure Resource Manager option, then scroll all teh way down and click Next. New service connections\rMake sure the Service principal (automatic) option is selected and click next. New Azure service connection\rLeave the scope set to Subscription. In the Subscription dropdown select the desired subscription. Leave the Resource group dropdown empty to set the Service Connection permission scope at whole subscription level or if lesser scope is desired select an appropriate resource group. Give the service connection a logical name and provide a proper description. Uncheck the checkbox Grant access permission to all pipelines as if left the service connection will be usable by all pipelines which can be a security risk. Ensure that the results from steps 6 to 10 look similar to the following image and click the Save button. Filled New Azure service connecti","date":"2023-01-20","objectID":"/posts/2023/how-to-create-a-new-azure-service-connection-in-azure-devops/:0:0","tags":["Azure DevOps","Fundamentals","New Service Connection"],"title":"How to create a new Azure Service Connection in Azure DevOps üìò","uri":"/posts/2023/how-to-create-a-new-azure-service-connection-in-azure-devops/"},{"categories":["Azure DevOps Fundamentals"],"content":"How to create a new Project in Azure devOps","date":"2022-11-15","objectID":"/posts/2022/how-to-create-a-new-project-in-azure-devops/","tags":["Azure DevOps","Fundamentals","New Project"],"title":"How to create a new Project in Azure DevOps","uri":"/posts/2022/how-to-create-a-new-project-in-azure-devops/"},{"categories":["Azure DevOps Fundamentals"],"content":"This is the 2nd post in the category Azure DevOps Fundamentals of the blog post series on working with Azure DevOps In this post the I will show how to create a new Project in Azure DevOps. Since it much more fun to do research and play with technology in collaboration with others, Wesley Camargo will be covering some of the topics on he‚Äôs blog, while other topics are covered by my here. Naturally the all related posts will be referenced between the two blogs. What is an Azure DevOps Project\rA place to establish multiple repositories for source code and to plan, track progress, and collaborate on building solutions with the repositories in question. Basically, a Project represents a fundamental container where you can store data and source code. Important\rSince a Project serves to isolate data stored within it it isn‚Äôt easy to to move data from one Project to another. Typically, this would result in loss of history associated with that data.\rNow that it‚Äôs clear what an Azure DevOps Project is, which arguments would support the creation of an Azure DevOps Project? While the answer to this question is often specific to the case at hand there are some general arguments arguments for creating a new Azure DevOps Project. To prohibit or manage access to the information contained within a Project to select groups To support custom work tracking processes for specific business units within your organization To support entirely separate business units that have their own administrative policies and administrators To support testing customization activities or adding extensions before rolling out changes to the working Project To support an open-source software Project Info\rWhen a Project is created, Azure DevOps automatically creates a single team of the same name. This is sufficient for small organizations. For enterprise-level organizations, it may be necessary to scale up and create more teams and projects. A single Azure DevOps Organization can hold up to a 1000 projects.\rHow to create an Azure DevOps Project\rPrerequisites\rThe these steps assume that the following prerequisites have been met: An Azure DevOps organization has already been created, if you haven‚Äôt create on by following the steps in my previous post How to create an Organization in Azure DevOps The logged in account is a member of the Project Collection Administrators group or have the collection-level Create new projects permission set to Allow If your organization is brand new, like for example one I created in my previous post How to create an Organization in Azure DevOps the main page of your Organization will show the Project creation screen. However if you already have one or more Azure DevOps Project present in your Organization you would need to use the New Project button to create even more projects :-). SBoth approaches are depicted in the following images. No prior Azure DevOps projects present in the Organization\rOne Azure DevOps Project already present in the Organization\rImportant\rIt seems to be a a bug in the user interface when creating the very first Project in a fresh Azure DevOps Organization. The Description field is missing and also the advanced options are not shown. Fortunately this bug can be solved by navigating to any other page within the Organization and then navigating back. For example, simply click on the Organization settings in the bottom left corner and then return to the home page of your Organization. You will notice that the the missing options are now visible. If everything went well the result should be similar to the next image.\rFixed bug with no prior Azure DevOps projects present in the Organization\rSince I am picking up where I left off in my previous post How to create an Organization in Azure DevOps the following steps will depict the step starting from a fresh Azure DevOps Organization. However before going into the actial steps there are a number 3 subjects that need understating to be able to make the best considered cho","date":"2022-11-15","objectID":"/posts/2022/how-to-create-a-new-project-in-azure-devops/:0:0","tags":["Azure DevOps","Fundamentals","New Project"],"title":"How to create a new Project in Azure DevOps","uri":"/posts/2022/how-to-create-a-new-project-in-azure-devops/"},{"categories":["Azure DevOps Fundamentals"],"content":"How to create an Organization in Azure DevOps","date":"2022-11-06","objectID":"/posts/2022/how-to-create-an-organization-in-azure-devops/","tags":["Azure DevOps","Fundamentals","New Organization"],"title":"How to create an Organization in Azure DevOps","uri":"/posts/2022/how-to-create-an-organization-in-azure-devops/"},{"categories":["Azure DevOps Fundamentals"],"content":"With this post am kicking off a blog post series on working with Azure DevOps. The subjects covered by this series are organized into three categories: Azure DevOps Fundamentals Azure DevOps Advanced Azure DevOps Expert Starting with the basic of basics of Azure DevOps Fundamentals: How to create an Organization in Azure DevOps? Since it much more fun to do research and play with technology in collaboration with others, Wesley Camargo will be covering some of the topics on he‚Äôs blog, while other topics are covered by my here. Naturally the all related posts will be referenced between the two blogs. For a quick guide you can also check out the video version of this post. What is an Azure DevOps Organization\rAn Organization within Azure DevOps can be considered a logical container used to organize and group projects. This logical container is the highest level at which the following types of configuration is applied; Managing projects Managing Permissions Setting up billing Global notifications Configuring Auditing Installing and managing extensions Configuring Azure Active Directory integration Managing Policies Setting up board process Global pipeline configuration Artifact storage configuration and billing Create an Azure DevOps Organization\rPrior to jumping ahead and creating an Organization it is strongly advised to consider planning an organizational structure and als to determine what type of account is most suitable for your implementation. Info\rAs of writing this post all organizations must be manually created via the web portal. There is no support for automated creation of Azure DevOps organizations.\rImportant\rA Microsoft account (of any type) is required prior to starting with the next steps\rWith that mentioned, lets start with the hands-on part and walk trough the required steps to create an Azure DevOps organization. Login into Azure DevOps https://go.microsoft.com/fwlink/?LinkId=307137 using a Microsoft account Login to Azure DevOps\rAfter a successful login you will be presented with the ‚ÄòGet Started with Azure DevOps‚Äô screen On the ‚ÄòGet Started with Azure DevOps‚Äô screen select your country/region and click the Continue button. This will be used to preselect a Microsoft region which you can change in the following steps. Select your country/region\rOn the next screen decide if you desire to receive information tips and offers and again click the Continue button. Receive information tips\rOn the ‚ÄòAlmost done‚Äô verify and adapt the following fields Almost done‚Ä¶\rThe name of the organization. It is possible to change the name at a later stage, however the name is used as part of the URL which could have impact on accessibility. quote\rCurrently, you can only use letters from the English alphabet in the Organization name. Start the Organization name with a letter or number, followed by letters, numbers or hyphens, and the name also must end with a letter or number.\rThe desired region. Make sure you are aware of any regional data sovereignty that might apply to your Azure DevOps Organization as choosing the the desired region will impact it. In Addition, moving an Organization from one region to another is possible but limitations apply Challenge validation When done, again press the Continue button The organization will be created, this usually takes a couple of minutes Taking you to you DevOps Organization\rWhen a new organization is created you are presented with the screen to continue by creating the first project. This is a separate topic which I will handle in the next post of the Azure DevOps Fundamentals category. For the creation of an Azure DevOps Organization all steps are done!!! Organization DevOps\rGet started managing your Azure DevOps Organization\rAfter creating the first Azure DevOps Organization it is important to consider finalizing the configuration of the following topics: Add users to your organization Set up billing Manage security and permissions Enable preview features for your organization Install an","date":"2022-11-06","objectID":"/posts/2022/how-to-create-an-organization-in-azure-devops/:0:0","tags":["Azure DevOps","Fundamentals","New Organization"],"title":"How to create an Organization in Azure DevOps","uri":"/posts/2022/how-to-create-an-organization-in-azure-devops/"},{"categories":["Cloud Governance","Naming Conventions"],"content":"Azure management group naming convention","date":"2022-09-26","objectID":"/posts/2022/the-ideal-management-group-naming-convention/","tags":["Naming convention","Azure","Management Group","Cloud","Resource naming"],"title":"The ideal Management group naming convention","uri":"/posts/2022/the-ideal-management-group-naming-convention/"},{"categories":["Cloud Governance","Naming Conventions"],"content":"A new post and a new addition to the naming convention series. With this post I want to share my approach to naming Azure Management groups. Tip\rIf you haven‚Äôt already, don‚Äôt forget to check out the other topics in the naming convention series: The 10 commandments for Azure naming conventions The perfect azure naming convention A naming convention to bring order to variable groups Git gud with branch naming What are Management groups and theirs limits\rIf you are wondering about reasons for this approach to naming please have a look at the Naming convention background chapter of the The perfect azure naming convention. With that out of the way lets dig in. Azure management groups are used to efficiently manage access, policies, and compliance by grouping subscriptions together. While a single directory supports up to 10.000 management groups, the management group tree can support up to six levels. This is excluding the root level. Because of the combination of the 6 level limit, the maximum of 10.000 per directory and the limit to a single directory there is an relatively simple and logical approach to naming management groups. And I am going to show it. Why is Azure directory abbreviation important\rFor organizations that have multiple directories it will be important to understand to which directory a certain Management group belongs. This would be most important from an automation point of view as the Azure portal already takes care of it by making you select a directory first. So the prefix would need to be the abbreviation of directory in question. Example: How to abbreviate Azure Management groups\rThe directory abbreviation is then followed by the Microsofts Management group abbreviation mg which is combined with a serial number. To make overall naming consistent this approach is similar to the The perfect azure naming convention which covers resource naming. This number is where it gets interesting. Since there is a limit of 6 layers it would be logical to actually number each layer by using the first digit as a layer indicator. Example with both a 4 and a 5 digit serial number: The length of the serial number depends greatly on the size of the organization and it‚Äôs directories. Info\rA 4 digit serial number would limit each Management group layer to 999 Management groups and a total maximum of 5994 Management groups. In most cases a small organization would suffice with a 3 digit serial number, a regular organization would suffice with a 4 digit serial number and an enterprise would likely need 5. The ideal Azure Management group naming convention\rCombining the defined prefix and suffix results the following Management group naming convention: Warning\rWhen a Management group is created two values are needed as input. A Management group ID and a Management group display name. When designing a naming convention make sure to keep in mind that the Management group ID cannot be modified after creation. This is luckely not the case for the Management group display name.\rAn example of a 3 digit serial number Management group structure\rWrapping up\rAnd there you have it, the ideal Management group naming convention. If you are interested in knowing more about management groups check out Microsoft‚Äôs What are Azure management groups? page at https://learn.microsoft.com/. And for more practical info on this subject check out the post from Wesley Camargo How to deploy Management Groups with Azure Bicep and Azure DevOps As always, a big thanks for reading this post. If you liked it, don‚Äôt be shy and have a look at my other posts. ","date":"2022-09-26","objectID":"/posts/2022/the-ideal-management-group-naming-convention/:0:0","tags":["Naming convention","Azure","Management Group","Cloud","Resource naming"],"title":"The ideal Management group naming convention","uri":"/posts/2022/the-ideal-management-group-naming-convention/"},{"categories":["Tooling"],"content":"Using Winget to install my favorite software.","date":"2022-08-30","objectID":"/posts/2022/getting-along-with-winget-advanced-installation/","tags":["winget","package management","development box setup"],"title":"Getting along with winget - advanced package installation","uri":"/posts/2022/getting-along-with-winget-advanced-installation/"},{"categories":["Tooling"],"content":"In my previous post Getting Along With Winget - Basic Packed Management I went trough my experiences with using the basic Winget commands. And mentioned that anyone who needs more control over the installation options will quickly note that when Winget uses the default installation options when installing packages. In this post I would like to share with you how I managed to solve this and get the control I need during the installation process. Winget Advanced installation, the override command\rThe winget install documentation shows that an --override option is available, with an input ‚ÄúA string that will be passed directly to the installer‚Äù. Simply said it should allow to override the default installation options with custom ones, similar to the way a regular installer GUI allows to the user to change settings during the installation process. A.k.a. a custom installation and exactly what I need! Unfortunately getting the the required input options for this command turned out to be lots of interwebs searches, Inno Setup documentation digging combined with trial and error. So the best approach I discovered so far is to run the installer with /? to get the installer help. As shown for the Git installer in the following image. Git installer with /? command line parameter result\rAnd simply going trough the available parameters and identifying the most interesting ones (marked in the above screenshot). /SILENT Instructs the installer to be silent \u003c- Enables installations that do not require settings to be specified via the installer GUI /SAVEINF Instructs the installer to save installation settings to the specified file /LOADINF Instructs the installer to load the settings from the specified file Saving and loading the custom installation options made sense so time to run with these parameters. First the save command combined with the Git installer. This resulted in the following .inf file. Content of GitInstallParams.inf\rInfo\rP.S. I went with thew .inf file extension because of the parameters being called /SAFEINF and /LOADINF. But as I understand any file extension should work here.\rSince the .inf file looked promising it was time to combine all the parameters into a single command line and running it, Combined command line: And as shown in the following screenshots running this command results in a successful installation! Installation in progress\rInstallation successful\rTo top it all off a quick check of the git config which seems to confirm a successful installation with the intended installation configuration. Verying Git installation config using ‚Äògit config ‚Äìlist‚Äô\rSuccess! I and hopefully whoever reads this post is now able to use Winget and still achieve custom installation configuration. Winget limitations\rIt should come as no surprise there are also downsides to both Winget in general and the explained approach with --override command. This chapter contains a short list of the ones I encountered. As of writing this post Winget only supports .zip installer in the preview build The winget upgrade --all command does not respect the configuration set with the --override (this behavior seems to differ per application) The bulk commands winget export and winget import don‚Äôt offer any support for the --override input, meaning that bulk installation can only be done with default installation options Winget lacks a auto update feature to automatically update all or a selected set of packages. Thankfully someone from the community stepped in and created one Winget-AutoUpdate Wrapping up\rAnd this is it for the the advanced installation of packages using Winget. I really love the simplicity, accessibility and the fact that Winget comes pre-installed on recent Windows machines. And it is because of this focus on basic simplicity installing packages using advanced commands feels a bit cumbersome. Compared to the other packge management tools Winget is still relatively new and being actively developed so who knows what upgrades this to","date":"2022-08-30","objectID":"/posts/2022/getting-along-with-winget-advanced-installation/:0:0","tags":["winget","package management","development box setup"],"title":"Getting along with winget - advanced package installation","uri":"/posts/2022/getting-along-with-winget-advanced-installation/"},{"categories":["Tooling"],"content":"Using Winget to install my favorite software.","date":"2022-08-22","objectID":"/posts/2022/getting-along-with-winget-basic-commands/","tags":["winget","package management","development box setup"],"title":"Getting along with winget - basic packed management","uri":"/posts/2022/getting-along-with-winget-basic-commands/"},{"categories":["Tooling"],"content":"During my summer holiday I had a one day break from sunny weather so I decided to use the family tablet to fiddle with some PowerShell. As expected only the standard software was installed on the tablet. I could either start installing everything I need manually or use this opportunity dig a bit into winget. I hope you will enjoy both the basics and the advanced configuration experience shared via this post. So, for folks who are unfamiliar with Winget, Winget or Windows Package Manager is a comprehensive package manager solution that consists of a command line tool and set of services for installing applications on Windows 10 and Windows 11. It is bundled with Windows 11 and modern versions of Windows 10 by default as the App Installer. Tip\rIn case it‚Äôs missing on your machine, simply download it via the Microsoft store. And also make sure you have the latest version by going into the Microsoft store and looking for any updates for the ‚ÄòApp Installer‚Äô.\rWinget basic commands, install and search\rThe very first step was to have a look at all the available commands for winget. To do so I simply opened Command Prompt and typed: winget --help The result is shown in the next screenshot. Result of ‚Äòwinget ‚Äìhelp‚Äôcommand\rIn my current case I only need to have Git and VSCode installed. Based on the available commands pulled up via winget --help I should either search for the packages using winget search or proceed to installing by using winget install. The install command winget install --help looks more promising as it says: Installs the selected package, either found by searching a configured source or directly from a manifest. By default, the query must case-insensitively match the id, name, or moniker of the package. Other fields can be used by passing their appropriate option. Result of ‚Äòwinget install ‚Äìhelp‚Äô command\rUsing the following lines lets give ‚Äòwinget install‚Äô a go: As shown in the next screenshot, by running these commands VSCode got installed directly but for Git I had to use winget install --id Git.Git as ‚ÄòGit‚Äô term returned multiple hits. Result of ‚Äòwinget VScode‚Äô and ‚Äòwinget install ‚Äìid Git.Git‚Äô commands\rMe being curious I decided to also check out the search command winget search \u003cappname\u003e. As shown in the next screenshot, it works like expected. Result of ‚Äòwinget search 7zip‚Äô command\rAs you can see both Git and VSCode got installed successfully. Successful installation!\rReference material and useful links\rUse the Winget tool Finding winget packages made simple The manifests for all winget packages Winget-AutoUpdate Example of /mergetasks alternative Using command line arguments Wrapping up\rAnd this is it for the basic usage of Winget. Winget is by default available on any up to date Windows 10/11 machine and in basic usage very accessible. Anyone who needs more control over the installation options will quickly note that when Winget uses the default installation options when installing packages. In my specific case this means that the wide array of installation options offered by Git will be all set to the installation defaults. For VSCode the ‚ÄúOpen with Code‚Äù context wont be set as the installation default does not have these checked. For those who are interested in how I managed to solve this and get the control I need during the installation process, make sure to read the next post getting along with winget - advanced installation. Thanks for reading this post and if you liked it don‚Äôt be shy and have a look at my other posts. ","date":"2022-08-22","objectID":"/posts/2022/getting-along-with-winget-basic-commands/:0:0","tags":["winget","package management","development box setup"],"title":"Getting along with winget - basic packed management","uri":"/posts/2022/getting-along-with-winget-basic-commands/"},{"categories":["Cloud Governance","Naming Conventions"],"content":"A Git branch naming convention","date":"2022-06-29","objectID":"/posts/2022/git-gud-with-branch-naming/","tags":["Naming convention","Azure DevOps","DevOps","Cloud","Branch","Branching","Git","Github"],"title":"Git gud with branch naming","uri":"/posts/2022/git-gud-with-branch-naming/"},{"categories":["Cloud Governance","Naming Conventions"],"content":"With this post I want to share my approach to Git branch naming. This is the 4th and final post in the naming convention series. Tip\rCheck out the full naming convention series: The 10 commandments for Azure naming conventions The perfect azure naming convention A naming convention to bring order to variable groups When working in large teams, managing branches can quickly become a hassle. Comparing branches to figure out their feature set, is a time consuming and error prone activity. This should be avoided as it significantly increases the chance of releasing wrong or even broken functionality. The most ideal solution here would be to use a backlog tool (for example Azure Boards with Azure Repos or with Github) where the branches can be directly linked to backlog items and visa versa. And dotting it i‚Äôs by implementing a proper branch naming convention. However teams that do not have these tools available to them would need to rely on just the naming convention to help organizing and ordering branches. A friend of mine recently looked into an Azure DevOps project and just a single repo had over 600 active branches (this sounds like a separate problem, I know). What really made it difficult is that also the branch naming was all over the place. Here is a small example of just the feature branch variations: In the following chapters lets have a look at the steps needed for the owners of this project to improve their situation with a naming convention. Establish branch types\rAlways start by identifying and agreeing on the type of branches your team will use as this can vary based on the preferred branching strategy. If unsure, generally speaking the following branches should cover most commonly used strategies. develop main hotfix feature release The branches with an infinite longevity like main, master, and development can be excluded from any naming convention as their names are set only once during their lifetime. However the branches of a short longevity and which exists in multitude (as mentioned in the earlier example) should be subjected to a naming convention. For these type of branches follow up by detailing out the approach to their naming. Let‚Äôs apply this on the above mentioned branches. Hotfix branch\rHotfix branches are always prefixed by ‚Äòhotfix‚Äô followed by a forward slash ‚Äò/‚Äô and finally the hotfix identifier. The hotfix identifier should always correspond to a backlog item number of the bug or hotfix in question. If the team prefers a more descriptive approach the name of the hotfix can be used as a suffix. However I am against just using the name without any identifier as teh name of a backlog item can be subjected to change An example: Feature branch\rFeature branches are always prefixed by ‚Äòfeature‚Äô followed by a forward slash ‚Äò/‚Äô and finally the feature identifier. Similar to the hotfix branch the feature identifier should always correspond to a backlog item number. And based on the teams preference a descriptive suffix can be added. An example: Note: The backlog item does not have to be of type feature as it is not a direct correlation to a backlog item of the type feature but a reference to a functionality feature. So it can be of any type e.g. task, user story or even a subtask. Release branch\rRelease branches are always prefixed by ‚Äòrelease‚Äô followed by a forward slash ‚Äò/‚Äô and finally the release version number. As a fan of the semantic versioning approach I like to use it when requirements and constraints allow for it. An example: Wrapping up\rAnd that is all I have to share on this topic. Thanks for reading this post and if you haven‚Äôt please check out my previous post on this subject. ","date":"2022-06-29","objectID":"/posts/2022/git-gud-with-branch-naming/:0:0","tags":["Naming convention","Azure DevOps","DevOps","Cloud","Branch","Branching","Git","Github"],"title":"Git gud with branch naming","uri":"/posts/2022/git-gud-with-branch-naming/"},{"categories":["Cloud Governance","Naming Conventions"],"content":"Azure DevOps variable group naming convention","date":"2022-06-15","objectID":"/posts/2022/a-naming-convention-to-bring-order-to-variable-groups/","tags":["Naming","Naming convention","Azure DevOps","DevOps","Cloud","Variable group"],"title":"A naming convention to bring order to variable groups","uri":"/posts/2022/a-naming-convention-to-bring-order-to-variable-groups/"},{"categories":["Cloud Governance","Naming Conventions"],"content":"With this post I want to share my approach to naming Azure DevOps variable groups. This post is the 3rd in the naming convention series. Tip\rCheck out the full naming convention series: The 10 commandments for Azure naming conventions The perfect azure naming convention Git gud with branch naming The naming convention\rIn my experience there are a lot of different way‚Äôs Azure DevOps projects are organized. Also in contrary to the Azure resource naming variable groups are renamable and don‚Äôt possess an ability to hold additional metadata like tagging. This changes the approach to naming to focus much more on meaningful and descriptive naming within the context of the Azure DevOps project. Therefore a truly holistic naming of variable groups is challenging to achieve. But abiding the The 10 commandments for Azure naming conventions should still be feasible. The solution to this challenge is to define a rationale from which the naming convention is derived. The rationale I use is described is as follows. Rationale\rThe name of a variable group is scoped to an individual Azure DevOps project. This keeps the naming convention simple as a major part is already handled by the project name and it‚Äôs description. The reason for starting with the product / team affix is to group the variable group‚Äôs by their concerning products and/or teams. Because some projects are used for hosting multiple products and are used by multiple teams it is important to be able to see at glance which variable groups are part of which products. So starting the name with the product / team affix references the product or team in question but also groups all variable groups. The second segment can either reference the pipeline name if the variable group in question is only and only used by the single pipeline in question. Otherwise the segment should contain the purpose of the variable group. To allow for meaningful naming the purpose can be widely interpreted. For example; a variable group which contains -ado- is purposed to host Azure DevOps variables. The final segment should contain the environment applicable to the variable group in question. The assumption here is that the variable groups are set-up to be environment specific. Combined the rationale comes together into the following naming convention: \u003cproduct-affix|team-affix\u003e-\u003c[product|purpose]\u003e-\u003cenvironment [exp|dev|qa|uat|prd]\u003e Note\rAs reminded to me by MingZh make sure to also check the Azure DevOps naming restriction.\rNaming convention in practice\rLets use the naming convention on the following use case and derive a couple of examples from it: Digital Solutions workload contains a Payment Processing Application as one of the products. This feature is developed by the DevOps team. In an Azure DevOps project named like the workload, Digital Solutions. Example 1: Product centric organized variable group naming \u003cproduct\u003e-\u003cpurpose\u003e-\u003cenv\u003e Example 2: Team organized variable group naming \u003cteam\u003e-\u003cpurpose\u003e-\u003cenv\u003e Wrapping up\rAnd that is all there is to tell about my approach to organizing variable groups trough a naming convention. Thanks for reading this post and if you haven‚Äôt please check out my previous post on this subject. I cant wait to share my next and final post in this series series. ","date":"2022-06-15","objectID":"/posts/2022/a-naming-convention-to-bring-order-to-variable-groups/:0:0","tags":["Naming","Naming convention","Azure DevOps","DevOps","Cloud","Variable group"],"title":"A naming convention to bring order to variable groups","uri":"/posts/2022/a-naming-convention-to-bring-order-to-variable-groups/"},{"categories":["Cloud Governance","Naming Conventions","Azure"],"content":"Azure resource naming convention","date":"2022-05-10","objectID":"/posts/2022/the-perfect-azure-naming-convention/","tags":["Naming convention","Azure","Resources","Cloud","Resource naming"],"title":"The perfect Azure naming convention","uri":"/posts/2022/the-perfect-azure-naming-convention/"},{"categories":["Cloud Governance","Naming Conventions","Azure"],"content":"This post is the 2nd in the naming convention series. With this post I want to share my approach to naming Azure resources. Why a naming convention is important is already covered by my previous post. However I do believe that some background is required to understand this approach. So, lets dig in. Tip\rCheck out the full naming convention series: The 10 commandments for Azure naming conventions A naming convention to bring order to variable groups Git gud with branch naming Naming convention background\rA proper naming convention is one of the main methods for building and maintaining Cloud infrastructure as cattle and not as pets. Supporting the transition from the traditional approach of treating infrastructure as pets ‚Äúunique snowflakes‚Äù with names and emotional attachments, to a Cloud native model whereby if a problem with infrastructure is found the infrastructure in question (or its configuration) is ‚Äúsimply‚Äù destroyed and replaced (infrastructure as code). This approach is a must to provision and manage the complex and rather large amount of components \u0026 configurations the current day Cloud resources are consisting of. In addition Azure resource names are immutable (meaning that they cannot be changed after creation) and in some cases not reusable for a set period of time (purge protection). So it is crucial to name resources with longevity in mind and to avoid extensive rework and downtime in later stages. Therefore the purpose of this this naming convention is to ensure resource names reflect a logical structure enabling automation, scripting and Infrastructure as Code to find, read and process this complex and large amount of resources \u0026 configurations with as little and simple logic as possible. Ensuring proper performance, low effort maintainability and drift control. Resource tagging, a naming conventions best friend\rBy prioritizing automation operators might get into trouble processing resources by their names. Therefore any naming convention should always be implemented in conjunction with a proper tagging strategy. Since most resources can be tagged with up to 50 resource tags which are mutable and are unlike the resource names not subjected to limiting naming rules and restrictions an operator is be able to filter, search, find and process resources based on their tags effortlessly (check out the following screenshot examples). Resulting in a best of both worlds scenario. And of course tagging is the first step to implementing cost management but thats a different story :). Example: Filtering resources Example: Search resources Example: Visual summary view Naming convention explained\rTechnically this naming convention arranges components sizewise which also the approach the metric system uses. To visualize this in a clear way for everyone disregarding individual knowledge of sizewise and the metric system, the components are arranged in the form of a stacking doll, a.k.a a Matroska or stacking dolls. Like the ones shown in the next image. Matroska / Stacking dolls\rStarting with the biggest component, and moving to an ever smaller component. Up to the point of the sub-resource as sub-resource is the smallest component for this naming convention. Some sub-resource examples: A network card part of a virtual machine A disk part of a virtual machine A managed identity of a resource Note\rAs mentioned in the previous chapter the naming convention has longevity of resources in mind. Some resource types can be configured for fail over and be recovered in a different region then the original one. Therefore the region (location) of a resource is not included in the naming convention. And, the region is a field that is present in almost all portal.azure.com views as it is one of the default properties of a resource object. Simply put: adding the region to the naming convention is overkill.\rThe order of components (Landing Zone, Solution, workload, application, environment, etc.) and in particular the position of the envir","date":"2022-05-10","objectID":"/posts/2022/the-perfect-azure-naming-convention/:0:0","tags":["Naming convention","Azure","Resources","Cloud","Resource naming"],"title":"The perfect Azure naming convention","uri":"/posts/2022/the-perfect-azure-naming-convention/"},{"categories":["Cloud Governance","Naming Conventions"],"content":"The basics for any naming convention in Azure","date":"2022-04-25","objectID":"/posts/2022/the-10-commandments-for-azure-naming-conventions/","tags":["Naming convention","Azure","Resources","Cloud","Resource naming"],"title":"The 10 commandments for Azure naming conventions","uri":"/posts/2022/the-10-commandments-for-azure-naming-conventions/"},{"categories":["Cloud Governance","Naming Conventions"],"content":"There are many Azure related naming conventions guides but this one is mine! For every 10 engineers and architects asked you will get 10 different replies on such a topic as naming conventions. With this blog post I would like to kick-off a small series of blog posts sharing my view points about Azure related naming conventions. In this post starting with why a proper naming convention is important and the 10 commandments I generally follow when working on a naming convention. Tip\rCheck out the full naming convention series: The perfect azure naming convention A naming convention to bring order to variable groups Git gud with branch naming Why is a proper naming convention important?\rA naming convention is a set of rules for choosing the character sequence to be used for identifiers which denote variables, types, functions, and other entities in source code and documentation. source - Wikipedia By adoption naming conventions on important and widely used area‚Äôs we strife to achieve the following benefits: To reduce the effort needed to read and understand source code. To enable code reviews to focus on issues more important than syntax and naming standards. To enable code quality review tools to focus their reporting mainly on significant issues other than syntax and style preferences. To provide additional information (i.e., metadata) about the use to which an identifier is put. To help formalize expectations and promote consistency within a development team. To enable the use of automated refactoring or search and replace tools with minimal potential for error. To enhance the aesthetic and professional appearance of work product. To provide meaningful data to be used in project handovers. To provide better understanding in case of code reuse after a long interval of time. The 10 commandments\rIt is important to ensure that automation is able to process objects without creating exceptions for parsing rules and at the same time all developers, engineers and architects should also be able to easily understand the names of all objects. Depending on the objects to which a naming convention applies the focus should either lean more towards automation or towards people (I will elaborate my view on this in the following blog posts of this series). But generally speaking this still should be be achievable by using the following 10 naming commandments. Names used should be in correct American English. Why American English? Well because that is the established international language for programming. Commonly accepted short forms or abbreviations of long words should be used for brevity. For example, API is preferred over Application Programming Interface. Use intuitive, familiar terminology where possible. For example, when describing removing (and destroying) a resource, delete is preferred over erase Use the same name or term for the same concept, including for concepts shared across subject domains. Avoid name overloading. Use different names for different concepts. Avoid overly general names that are ambiguous in a larger ecosystem. They can lead to misunderstanding of objects. Rather, choose specific names that accurately describe the object. For example, when script that automates a number of test actions don‚Äôt call it automation call it specific like test-automation Carefully consider use of names that may conflict with keywords in common programming languages. Such names may be used but will likely trigger additional scrutiny during review. Use them judiciously and sparingly. Ensure that the chosen case type (kebab-case-example, snake_case_example, camelCaseExample, PascalCaseExample, etc) is usable in as many systems, products and tools and stick with it. For example, some web-based systems don‚Äôt allow snake_case as it uses underscores and there are others that don‚Äôt allow for hyphens making kebab-case undesirable. Avoid using organizational structures as departments or business units of companies \u0026 organizations in a naming con","date":"2022-04-25","objectID":"/posts/2022/the-10-commandments-for-azure-naming-conventions/:0:0","tags":["Naming convention","Azure","Resources","Cloud","Resource naming"],"title":"The 10 commandments for Azure naming conventions","uri":"/posts/2022/the-10-commandments-for-azure-naming-conventions/"},{"categories":["Microsoft 365"],"content":"How to rename your SharePoint Online url","date":"2022-03-30","objectID":"/posts/2022/change-the-sharepoint-domain-name-in-your-microsoft-365-tenant/","tags":["Domain","Root url","Tenant","M365","Rename","Update","SharePoint Online","Office 365"],"title":"Change the SharePoint domain name in your Microsoft 365 Tenant","uri":"/posts/2022/change-the-sharepoint-domain-name-in-your-microsoft-365-tenant/"},{"categories":["Microsoft 365"],"content":"Back when all the Cloud and SaaS offerings where a brand new concept most IT administrators ended up choosing the domain name of a tenant. This choice would be then reflected in the SharePoint Online url and the only way to change it was to create brand new tenant. Either due to the somewhat poor choice from the IT administrators, due to mergers, rebranding or acquisitions some organization ended up with lets say less desirable domain name for their SharePoint URL. Responding to clients with a ‚Äúno it is not possible to change that‚Äù always bothered me back then. So when I learned that this option is now available (in preview) I immediately decided to try it. Via this post I want to share what I learned testing this option. Please make sure to read the official documentation for limitations of this option and the fact that this is till in preview at the time of writing this post before even considering it in production. Having said that, lets go! Start by navigating to https://aka.ms/SPORenameAddDomain. Make sure to use this URL. If you decide to navigate without the use of this URL the rename will not work. Click on Add a custom domain button at the top, then fill in the desired domain name including the .onmicrosoft.com suffix. As shown in the following image I want to rename my tenant from 2tkc6c to demojev there fore I used demojev.onmicrosoft.com. Note\rIf you have to try and figure out if certain domains might be available you can just enter those domains in your browser using this format https://[YOUR_DOMAIN_NAME].sharepoint.com.\rDomain name added page with warning\rClick on the Add domain button at the bottom right corner. If the domain is available you will get a message Domain name added and will be redirected to screen show in the following image. Note\rDo not add any other domains and make sure you did not get a prompt create a new TXT record with your domain name registrar. Because if you did the domain name has not been added correctly and it will not be elegible for a rename. If you get this prompt you need to delete the newly added domain and recreate it again starting from step 1.\rAdd a custom domain name page\rAs show in the following image use the top navigation breadcrumb to go back to the custom domain names. Breadcrumb, use it to go back\rEnsure that the newly added domain name is verified. This is shown in the following image. New domain name is verified\rFor continuing with the next steps the latest version of the SharePoint Online Management Shell is required. Make sure to have it installed. Install SharePoint Online Management Shell\rOpen a PowerShell 5.1 window and connect using the newly downloaded and installed SharePoint Management Shell by using the Connect-SPOService command using your current SharePoint domain. Depicted from my own rename in the following PowerShell snippet. A log-in prompt will be shown, use a Global Administrator account to complete this log-in. As again shown in the following image. Log-in prompt\rThe last remaining step is to queue the rename. Since the rename must take place at least 24h in the future I decided to add a timespan of 24 hours and 10 minutes to the current date \u0026 time. The combined commands are shown in the following PowerShell snippet. Example of the script outputs: Check rename status script output\rThe command returns a RenameJobID but at this moment this ID cannot be used as input anywhere. The documentation states that the status of the rename can be acquired through the command Get-SPOTenantRenameStatus. However since it‚Äôs still in preview the following note is stated when navigating to the documentation: This feature is in preview and currently available to organizations that have no more than 1,000 total SharePoint sites and OneDrive accounts combined. Fast forward 24 hours and 10 minutes and the results of the rename should visible. Successful domain rename rename\rAnd that is it. Thanks for reading this post and if you liked it don‚Äôt be shy and have a look at m","date":"2022-03-30","objectID":"/posts/2022/change-the-sharepoint-domain-name-in-your-microsoft-365-tenant/:0:0","tags":["Domain","Root url","Tenant","M365","Rename","Update","SharePoint Online","Office 365"],"title":"Change the SharePoint domain name in your Microsoft 365 Tenant","uri":"/posts/2022/change-the-sharepoint-domain-name-in-your-microsoft-365-tenant/"},{"categories":["Security","Azure DevOps","Azure"],"content":"A demonstration of service connection credentials exfiltration through an Azure DevOps pipeline","date":"2022-03-12","objectID":"/posts/2022/your-service-connection-credentials-are-mine/","tags":["Azure DevOps","Pipelines","Hack","Service connection","Zero Trust","Credentials","Service Connection security","Pipeline permissions"],"title":"Your service connection credentials are mine","uri":"/posts/2022/your-service-connection-credentials-are-mine/"},{"categories":["Security","Azure DevOps","Azure"],"content":"Like with the two previous posts Hacking Azure DevOps and I am in your pipeline reading all your secrets! I want to continue to raise awareness and understanding about pipeline security in Azure DevOps. In the previous post I have explained how secure / marked as secret variables are handled during pipeline runtime. In this post I want to show how an Azure Resource Manager service connection configuration is handled during pipeline runtime. And which sensitive information is exposed through this service connection. Like I mentioned in the previous posts, without proper security configuration for pipelines this information could be abused by attackers. But lets start with creating an out of the box Azure Resource Manager service connection. New Service Connection\rIn your Azure DevOps project click on the setting icon in the bottom left corner. Next in the setting menu select the service connection option. On the new page in the top right corner click on the New Service Connection button. A list of the available service connection type should be visible on you right hand side. Select the Azure Resource Manager as shown in the following image and click next. New service connection - Azure resource manager\rNext, as shown in the following image select the recommended option Service principal (automatic) and click next. New service connection - select automatic\rOn the next page you should be able to see the subscriptions and resource groups to which your current account has access. Select the preferred subscription and resource group and click save. It should look as follows. New Azure service connection\rNote\rYou need at least owner permissions on a single resource group and your user must be able to register applications in the Azure AD to which your subscription is linked. In case more trouble shooting is required please have a look at MS docs Troubleshoot ARM service connections\rWhen done correctly the newly created service connection should be visible in the list. See following example. Successfully created service connection\rService connection un the hood\rNow that we have our service connection created, lets have a look under the hood. Click on the newly created service connection and the click on the managed service principal option as show in the following image. New service connection properties\rYou should be redirected to Azure AD page of the underlying App Registration as show in the following image. New service connection principal\rTo facilitate the service connection an App registration is created under the identity of the currently logged in account. We will ignore the awful looking naming that has been done to the App registration ;-). In addition a secret is also created to facilitate the use of this App registration by the service connection we just created in Azure DevOps. Click on the Certificates and secrets option in the left hand menu, the secret in question should be visible. Naturally the secret value is hidden. This is also show in the following image. New service connection principal secret\rIn addition this app registration is also added as contributor to the resource group or any other scope that is selected during the creation process. In the case of this example I have selected the resource group called demo-ado-scraper. Have a look at the following image. New service connection principal permissions\rSo an App registration with a secret is created and contributor permissions are applied to the selected scope. Meaning that if it would be possible to obtain the App registration details including secret from within a pipeline, it would allow an attacker to at the very least gain contributor permissions over all the resources available under the selected permissions scope. In my case that would be the resource group. Lets have have a look how we can achieve this. Getting the App registration secret\rIdeally to get any kind of authentication and authorization going the following values would be required. Secret","date":"2022-03-12","objectID":"/posts/2022/your-service-connection-credentials-are-mine/:0:0","tags":["Azure DevOps","Pipelines","Hack","Service connection","Zero Trust","Credentials","Service Connection security","Pipeline permissions"],"title":"Your service connection credentials are mine","uri":"/posts/2022/your-service-connection-credentials-are-mine/"},{"categories":["Security","Azure DevOps"],"content":"Demo of how a pipeline can be abused to exfiltrate secrets from variables and why variable groups and pipeline security is important","date":"2022-01-30","objectID":"/posts/2022/i-am-in-your-pipeline-reading-all-your-secrets/","tags":["Pipelines","Hack","Security","Variable Exfiltration","Secret Exfiltration","Zero Trust"],"title":"I am in your pipeline reading all your secrets!","uri":"/posts/2022/i-am-in-your-pipeline-reading-all-your-secrets/"},{"categories":["Security","Azure DevOps"],"content":"Introduction\rWith this blog post I want to raise awareness and understanding on how secure / marked as secret variables are handled during pipeline runtime in Azure DevOps and how these can be potentially exfiltrated. If proper security configuration is not in place this could potentially be abused by attackers. Lets move ahead to create different types of variables and try to retrieve their values. By doing so at the end of this blog post it will be clear why it‚Äôs not very sensible to give all project team members full access to pipelines. And why in some cases it‚Äôs better to set-up private build agents. Available variables\rCurrently Azure DevOps support the following ways to deal with variables. Write variables inline inside the YAML - Inline variables Set variables when starting a pipeline run - Queue time variables Set variables during pipeline creation - Pipeline variables Store variables in variable groups - Library group variables Link Key Vault to a variable group - Key Vault linked variables Since access to git repositories is considered standard for a user with contributor permissions and queue time variables only exists for a single run it does not make much sense to demonstrate these two types. Next step is to create the remaining four types. Creating variables\rTo create all four mentioned variables we are going to use an account with project administrator permissions. Making sure we are creating everything from an administrator perspective. Pipeline variables\rFirst the pipeline variables. These variables are set during creation or editing of a pipeline. Either on an existing pipeline or for a new one we can simply click on the variables button to bring the variable window up and click on New Variables as shown in the following images. New pipeline variables button\rFor this example the following variable is added and marked as secret. New pipeline variables\rLibrary group variables\rLibrary group variables are part of a variable group. Simply create a variable group by clicking on Variables and then +Variable group. For this example the following variable is added and again marked as secret. Library group variables\rKey Vault linked pipeline variables\rBasically this is the same set-up as the regular library group variables but this library group is linked with Key Vault. Meaning that selected Key Vault values can be managed in a Key Vault while used via the variable group in question. For this example I have created two variables inside a demo Key Vault and linked them to the variable group as follows. Two Key Vault secrets\rKey Vault variable group\rRuntime magic\rTo try and read the variables we are going to switch to a project contributor user. This user has the regular out of the box project contributer permission but with a bit stricter variable group permissions. The two variable groups used in the previous steps have had their permissions changed so that project contributers don‚Äôt have access to these groups. In the following image the difference in permission to variable groups is shown. Variable groups permission\rSo from a project contributor perspective prior to be able to read any variables we need to first find out which groups with which variables are present within the project. This can be done by calling the Azure DevOps API from within a pipeline as within the pipeline we can use the SYSTEM_ACCESSTOKEN of the project or even the project collection Build Service. This service is used during pipeline runtime so by default it has quite a large number of permissions. The script to get what we need looks als following. Result of this script looks as follows. All variable group secrets\rLooking at the script output we can see that the variable group my-demojev-vars contains a marked as secret (‚ÄúisSecret‚Äù: true) variable called very-secret-secret and that the variable group my-shuttle-kv-vars has test-keyvault-secret one and two variables marked as secret. Note\rVariable values of variables which are not marked as","date":"2022-01-30","objectID":"/posts/2022/i-am-in-your-pipeline-reading-all-your-secrets/:0:0","tags":["Pipelines","Hack","Security","Variable Exfiltration","Secret Exfiltration","Zero Trust"],"title":"I am in your pipeline reading all your secrets!","uri":"/posts/2022/i-am-in-your-pipeline-reading-all-your-secrets/"},{"categories":["Security","Azure DevOps"],"content":"Demo of a rather unconventional \u0026 unexpected Lateral Movement Attack which utilizes a possible configuration vulnerability in an Azure DevOps Organization","date":"2021-05-19","objectID":"/posts/2021/hacking-azure-devops/","tags":["Azure DevOps","Pipelines","Hack","Security","Lateral Attack","Unconventional"],"title":"Hacking Azure DevOps","uri":"/posts/2021/hacking-azure-devops/"},{"categories":["Security","Azure DevOps"],"content":"Introduction\rWhile this case is not a particularly new one and has been posted by Matt Cooper on devblogs.microsoft.com back in August 2020. I still feel that in relation to the possible data spillage it has not received sufficient exposure and the correct amount of awareness I would have expected. I actually stumbled upon this case by accident when playing with the Azure DevOps Library variables API. So in this post I want to showcase how a possible attacker can use a compromised developers environment to gain access to almost all the data present in an Azure DevOps Organization. While access of the developers environment in question is limited to just a single Azure DevOps Project. Note\rThis showcase assumes that the Azure DevOps organization has been created prior to May 2020 and that the administrators of the Azure DevOps Organization in question have not enabled the settings which resolve this issue. So you have probably have nothing to worry about if your Azure DevOps organization is from after May 2020 or if your admins already enabled the fix\rGoal\rSo what‚Äôs the goal of this post? The goal of this post is to emphasize the importance of zero trust and least privilege. And create awareness that security is everyone‚Äôs job. How am I going to achieve this goal? By demonstrating a rather unconventional \u0026 unexpected Lateral Movement Attack which utilizes a possible configuration vulnerability in an Azure DevOps Organization. Current situation\rFor this showcase I have created an example Azure DevOps Organization called DemoJev. This organization is currently populated by 5 projects. As shown in the following image. Example ADO organization\rLets assume that there are 3 developers Jan, Abdul and Henk who are working on the My Shuttle Project. They have access to: The project The code repo The variable group(s) The pipeline(s) And indirect access to the Azure resources deployed by the pipelines. The Azure permissions and configuration are actually not relevant within this showcase and are just added to draw a complete picture of a minimalistic set-up for a cloud native application development set-up. The drawn picture is as follows. Example ado project\rSince it is not the question if an environment is compromised but when. It is possible to assume that the development environment of one of the developers will eventually be compromised. In this showcase the development environment of Henk has been compromised. Henk is hacked!\rIt seems he is :-( Henk is hacked\rTo better understand the implications of the hack lets zoom in on Henk‚Äôs permissions. Like stated in the previous chapter Jan, Abdul and Henk are all team members on the project and thus it‚Äôs save to assume that all of them have contributor role within the My Shuttle project. Assuming that out of the box permissions of the Azure DevOps project are present, Henk should at least have the following permissions; Modify source code (partially depending on branch policies) Create \u0026 delete Git repos Create, delete, update and run pipeline Create, delete and update variable groups Modify board information Expected data spillage\rJudging from the above noted permissions it is presumable to expect that the attacker acquired the same level of access as Henk has. So letting Azure aside (again, not important for this case) the data spillage should be limited to the MyShuttle Azure DevOps project. As illustrated in the following image. Expected spillage\rActual data spillage\rContrary to the expected data spillage the actual data spillage is much larger. By fiddling with som Azure DevOps API‚Äôs and a bit of PowerShell scripting I was able to produce a data spillage the size of the whole Organization. See following the following image. Actual spillage\rSo a knowledgeable attacker can basically gain read access to pretty much all the data stored within the Azure DevOps Organization, including; All Azure DevOps projects within the same Organization All repositories in all Azure DevOps projects with","date":"2021-05-19","objectID":"/posts/2021/hacking-azure-devops/:0:0","tags":["Azure DevOps","Pipelines","Hack","Security","Lateral Attack","Unconventional"],"title":"Hacking Azure DevOps","uri":"/posts/2021/hacking-azure-devops/"},{"categories":["Comparison","Azure DevOps","GitHub"],"content":"A high level comparison between GitHub and Azure DevOps","date":"2020-11-06","objectID":"/posts/2020/github-vs-ado/","tags":["GitHub","Azure DevOps","Versus","Comparison","High Level"],"title":"GitHub vs Azure DevOps","uri":"/posts/2020/github-vs-ado/"},{"categories":["Comparison","Azure DevOps","GitHub"],"content":"Introduction\rRecently one of our clients has asked if it might be worthwhile to look into GitHub. They are currently using Azure DevOps and are as far as I understand quite happy with it. So, I drew up a short summary of pro‚Äôs and cons about GitHub vs Azure DevOps. The response that I got was very enthusiastic. And because of it I decided to share this information here. Keep in mind that this is not a full in depth comparison of both products but more of a management summary intended to decide if it would be valuable to look into what GitHub can offer a company that is currently using Azure DevOps. GitHub vs Azure DevOps\rSince the day Microsoft acquired GitHub they have started working towards brining these tools closed to each other. My personal opinion is that in the long run they will be merged and a single tool will remain. This can be seen in Microsoft‚Äôs imagery about their product positioning. A good example is the following image used in one of the blogs posts on Microsoft Developer Blogs. GitHub product positioning\rIn addition Medium.com has a great article on the same subject where Sasha Rosenbaum (GitHub product manager) is quoted stating that in the end there will be one product. However it will take a long long time. Please not that the pro‚Äôs listed in this chapter are not all applicable to all pricing plans. Generic pro‚Äôs\rMicrosoft‚Äôs own (public) source code is published in GitHub. GitHub has a huge and quite active community. GitHub has a far larger and more versatile offer of add-ins on it‚Äôs Marketplace. GitHub integrates nearly seamless with Azure DevOps, to the point that its even possible to use Azure DevOps board backlog items directly in GitHub. Like Azure DevOps GitHub also come equipped with Azure integration via Azure Pipelines. Most notable pro‚Äôs\rGitHub has dependency scanning where you get notifications and automatic pull requests when you have dependency vulnerabilities in third party packages source. GitHub has secret scanning that watches public and private (currently in beta for private repos) repositories for known secret formats and immediately notifies either the secret provider or private repository admins when secrets are found source. GitHub has the ability to find security issues as you code source. GitHub has GitHub Actions, an extensive workflow automation solution with CI/CD incorporated into it. It‚Äôs like CI/CD on steroids. GitHub has GitHubPages which is a feature to host and generate static HTML web pages from markdown documentation that is hosted inside your GitHub repositories. Cons\rIn‚Äôs unclear where GitHub stores the data geographically (although I did not reach out to GitHub to ask this directly), this could be an issue for some organizations from a compliance perspective. Native backlog features of GitHub are ok for a small/simple project but are lacking functionality for Scrum based team. This can be solved by purchasing a more advanced 3rd party solution from GitHubs marketplace. Single sign-on \u0026 automatic user provisioning features are only available as part of the Enterprise plan while Azure DevOps fully integrates with Azure Active Directory with a couple of simple clicks even in the ‚Äúfree‚Äù version. Final mention\rThe security features mentioned as part of the pro‚Äôs are only available as part of the Enterprise plan. Azure DevOps also has a set of security features available as part of the Microsoft Security Code Analysis product. However these must be purchased separately via a Microsoft partner. Since this is not an in depth comparison I did not take the time to compare the security features. Perhaps I might do so in a separate blog post. Conclusion\rI love the way GitHub has been able to position itself, it‚Äôs well know within the technical community and as an open source hosting platform while at the same time offering Enterprise grade solutions for the Enterprise market. Integration with Azure DevOps is done really well. Combining best from both products, allowing companies","date":"2020-11-06","objectID":"/posts/2020/github-vs-ado/:0:0","tags":["GitHub","Azure DevOps","Versus","Comparison","High Level"],"title":"GitHub vs Azure DevOps","uri":"/posts/2020/github-vs-ado/"},{"categories":["How to","GoHugo","GitHub","Azure DevOps"],"content":"A simple approach to automating a completely free an low maintenance blog using Azure DevOps, Hugo \u0026 GitHub Pages","date":"2020-09-10","objectID":"/posts/2020/free-blog-with-ado-and-github-pages/","tags":["GitHub","Azure DevOps","Versus","Comparison","High Level"],"title":"Set by step guide for a free and easy way to set up a low maintenance blog","uri":"/posts/2020/free-blog-with-ado-and-github-pages/"},{"categories":["How to","GoHugo","GitHub","Azure DevOps"],"content":"Introduction\rI always struggled with the following; when I find a solution for a certain problem and months down the line I run into a similar problem and try to remember how I solved the previous one. Cursing at myself: Why didn‚Äôt I write this down!? Like many other IT professionals I created a blog (years ago) to write those solutions down for myself and anyone who stumbles upon the blog. But unfortunately my blog at that time has followed the trend of the general population of IT related blogs. It withered away due to the lack of attention from yours truly. However the need to somehow catalog IT related solutions and ideas has not died down. This is specially the case during moments when I needed it the most and could not find the link or the solution itself knowing I solved the issue at hand a couple of month ago. Although on multiple occasions I have entertained the idea of recreating a new blog but never put it into action simply due to the amount of effort required for maintaining a blog. After my first blogging experience the CMS like systems with extensions and add-ins and the tools for writing the content itself never appealed to me and kept me from giving it another go. A couple of months ago I discovered static site generators. The idea of generating a static site from Markdown really appealed to me. Markdown is something I have used for years as part of documentation for the scripts and solutions I have developed. Markdown makes writing documentation approachable and inviting. In my case this approach appeals to me even more as I can use Visual Studio Code for writing blog content. Combined with a CICD pipeline which takes care of building and posting the content motivated me to give a blog another go. I am eager to share any solutions I find with the future me and anyone who finds the blog. I cant think of a more fitting subject to start the blog with then a step by step guide describing how I created this blog. Requirements\rThese are the tools I used to realize my current blog set-up. At the core I am using Hugo as my static content generator and GitHub Pages for hosting the static content. All other tools are supplemental to realize the core set-up and can be replaced by alternatives. VS Code Chocolatey Git Hugo Azure DevOps Organization - create one for free here GitHub Pages - create a GitHub account here Domain name (optional) -\u003e You should The set-up\rKeep in mind that my goals for this set-up are: It must be low threshold from a usage perspective. The maintenance effort must be zero to none. As already mentioned in the Requirements chapter I am using Hugo as the static content generator to meet my first goal. To meet my second goal I am running everything inside an Azure DevOps pipeline which publishes the generated blog to my GitHub account. The overall set-up looks like this: Overall view\rThis set-up allows for a trigger based release of content. So each time I push changes into the master branch the pipeline kicks in and ensures that the change is pushed to the output GitHub repository. The pipeline to achieve this is as follows: Pipeline view\rAs mentioned in the title I will do my best to explain the set-up step by step. However if a manual already exists for a specific part of configuration I will simply reference it (via a link). To structure this guide I have created a separate chapter for each step. In addition I have also split the really big steps into subchapters. The result is as follows: Introduction Requirements The set-up GitHub Pages repository Azure DevOps - Hugo blog repository Azure DevOps project CICD repository Hugo Installation and Configuration Azure DevOps - Yaml pipeline Checkout Run HugoTask@1 Publish Artifact Checkout Download Run GitHubPagesPublish@1 Add YAML file as pipeline in Azure DevOps Domain name configuration (optional) GitHub Pages repository\rNOTE: You need to have a GitHub account before you can continue with this chapter. You can create one here. Head over to GitHub an","date":"2020-09-10","objectID":"/posts/2020/free-blog-with-ado-and-github-pages/:0:0","tags":["GitHub","Azure DevOps","Versus","Comparison","High Level"],"title":"Set by step guide for a free and easy way to set up a low maintenance blog","uri":"/posts/2020/free-blog-with-ado-and-github-pages/"},{"categories":[],"content":"About the author, Jev Suchoi","date":"0001-01-01","objectID":"/about/","tags":["About me"],"title":"About","uri":"/about/"},{"categories":[],"content":"Experience\rAs a Cloud Architect with a hands-on approach, my journey started in childhood, taking apart toys with my dad‚Äôs tools to uncover the secrets of their inner workings. This early curiosity has developed into a professional passion to explore and deconstruct Cloud technology, ensuring a deep understanding of its core aspects to tailor solutions for diverse companies and organizations. Always keeping pace with the latest Microsoft technologies to deliver bleeding-edge solutions. I share insights, best practices, and the latest trends in Cloud technology by blogging, speaking at community events and on YouTube. Through these platforms, I aim to demystify technology and inspire others to innovate and excel in the ever-evolving landscape of Cloud services. I will always find time to talk at any community event via any media so don‚Äôt hesitate to drop me a message. My subjects of passion and focus: Microsoft Azure Azure DevOps \u0026 GitHub Microsoft Cloud Adoption Framework Azure Well-Architected Framework Cloud Center of Excellence/Enablement (CCoE) Infrastructure as Code (Bicep, Terraform) PowerShell Engineering Excellence Platform Engineering Authors note\rThis is a personal blog. The views and opinions expressed here are solely those of the author and do not represent those of any organization or any individual with whom the author may be associated, professionally or personally. Any solutions, scripts, or guidance are provided as-is with no warranty and are used at the risk of the user. Additionally, this blog leverages AI technology to enhance content creation and provide more insightful and accurate information. While AI assists in generating and refining content, all final views and opinions are personally reviewed and approved by the author. ","date":"0001-01-01","objectID":"/about/:0:0","tags":["About me"],"title":"About","uri":"/about/"}]